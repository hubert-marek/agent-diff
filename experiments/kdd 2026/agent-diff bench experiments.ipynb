{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install agent-diff langchain langchain-openai langchain-anthropic pandas matplotlib -q "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ca31907"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "#AGENT_DIFF_API_KEY = \"\" #Not needed for local server.\n",
        "#AGENT_DIFF_BASE_URL = \"\" #Only needed if using remote agent-diff server, for local it automatically uses local server at 8000.\n",
        "OPENAI_API_KEY = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Base path relative to this notebook\n",
        "DOCS_BASE = Path(\"../../examples\")\n",
        "\n",
        "def load_api_docs(filepath: Path) -> dict:\n",
        "    \"\"\"Load API docs JSON, return empty dict if not found.\"\"\"\n",
        "    if filepath.exists():\n",
        "        return json.load(open(filepath))\n",
        "    print(f\"Docs not found: {filepath}\")\n",
        "    return {}\n",
        "\n",
        "def format_docs_markdown(docs: dict) -> str:\n",
        "    \"\"\"Convert API docs dict to markdown format.\"\"\"\n",
        "    if not docs:\n",
        "        return \"\"\n",
        "    \n",
        "    markdown = \"\"\n",
        "    for endpoint, info in docs.items():\n",
        "        markdown += f\"## {endpoint}\\n\"\n",
        "        markdown += f\"{info.get('description', '')}\\n\\n\"\n",
        "        \n",
        "        if info.get('parameters'):\n",
        "            markdown += \"**Parameters:**\\n\"\n",
        "            for location, params in info['parameters'].items():\n",
        "                markdown += f\"  {location}:\\n\"\n",
        "                if not isinstance(params, dict):\n",
        "                    markdown += f\"    {params}\\n\"\n",
        "                    continue\n",
        "                for param_name, param_info in params.items():\n",
        "                    if not isinstance(param_info, dict):\n",
        "                        markdown += f\"    - `{param_name}`: {param_info}\\n\"\n",
        "                        continue\n",
        "                    required = \"**required**\" if param_info.get('required') else \"optional\"\n",
        "                    param_type = param_info.get('type', 'any')\n",
        "                    param_desc = param_info.get('description', '')\n",
        "                    markdown += f\"    - `{param_name}` ({param_type}, {required}): {param_desc}\\n\"\n",
        "            markdown += \"\\n\"\n",
        "    \n",
        "    return markdown\n",
        "\n",
        "# Load available docs (all services)\n",
        "slack_docs = load_api_docs(DOCS_BASE / \"slack/testsuites/slack_docs/slack_api_full_docs.json\")\n",
        "box_docs = load_api_docs(DOCS_BASE / \"box/testsuites/box_docs/box_api_full_docs.json\")\n",
        "calendar_docs = load_api_docs(DOCS_BASE / \"calendar/testsuites/calendar_docs/calendar_api_full_docs.json\")\n",
        "linear_docs = load_api_docs(DOCS_BASE / \"linear/testsuites/linear_docs/linear_api_full_docs.json\")\n",
        "\n",
        "# Format to markdown\n",
        "slack_docs_markdown = format_docs_markdown(slack_docs)\n",
        "box_docs_markdown = format_docs_markdown(box_docs)\n",
        "calendar_docs_markdown = format_docs_markdown(calendar_docs)\n",
        "linear_docs_markdown = format_docs_markdown(linear_docs)\n",
        "\n",
        "# Summary\n",
        "print(f\"[{'OK' if slack_docs else 'MISSING'}] Slack docs: {len(slack_docs)} endpoints\")\n",
        "print(f\"[{'OK' if box_docs else 'MISSING'}] Box docs: {len(box_docs)} endpoints\")\n",
        "print(f\"[{'OK' if calendar_docs else 'MISSING'}] Calendar docs: {len(calendar_docs)} endpoints\")\n",
        "print(f\"[{'OK' if linear_docs else 'MISSING'}] Linear docs: {len(linear_docs)} endpoints\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "# Service configurations with base URLs\n",
        "SERVICE_CONFIG = {\n",
        "    \"slack\": {\n",
        "        \"name\": \"Slack\",\n",
        "        \"base_url\": \"https://slack.com/api\",\n",
        "        \"description\": \"Slack workspace messaging and collaboration API\",\n",
        "    },\n",
        "    \"box\": {\n",
        "        \"name\": \"Box\",\n",
        "        \"base_url\": \"https://api.box.com/2.0\",\n",
        "        \"description\": \"Box cloud storage and file management API\",\n",
        "    },\n",
        "    \"calendar\": {\n",
        "        \"name\": \"Google Calendar\",\n",
        "        \"base_url\": \"https://www.googleapis.com/calendar/v3\",\n",
        "        \"description\": \"Google Calendar scheduling and events API\",\n",
        "    },\n",
        "    \"linear\": {\n",
        "        \"name\": \"Linear\",\n",
        "        \"base_url\": \"https://api.linear.app/graphql\",\n",
        "        \"description\": \"Linear project management and issue tracking API\",\n",
        "    },\n",
        "}\n",
        "\n",
        "# ReAct System Prompt \n",
        "REACT_SYSTEM_PROMPT_WITH_API_DOCS = \"\"\"You are an AI assistant that completes tasks by interacting with APIs via bash commands.\n",
        "\n",
        "## Current Session\n",
        "- **Service**: {service_name}\n",
        "- **Base URL**: {base_url}\n",
        "- **Description**: {service_description}\n",
        "\n",
        "## Environment\n",
        "- You are authenticated as a user in the {service_name} workspace/account.\n",
        "- Authentication is handled automatically via proxy. Use placeholder tokens like `<TOKEN>` where credentials would go.\n",
        "- You execute bash commands (primarily curl) to interact with the {service_name} API.\n",
        "- The environment is stateless between commands - you cannot install packages or persist files.\n",
        "\n",
        "## Response Format\n",
        "You must respond using XML tags. Think step-by-step, then execute a command OR declare completion.\n",
        "\n",
        "**To execute a bash command:**\n",
        "<thinking>\n",
        "Your reasoning about what needs to be done and why this command will help.\n",
        "</thinking>\n",
        "\n",
        "<action>\n",
        "Your bash command here (e.g., curl request)\n",
        "</action>\n",
        "\n",
        "**When the task is complete:**\n",
        "<thinking>\n",
        "Your reasoning confirming the task is done based on API responses.\n",
        "</thinking>\n",
        "\n",
        "<done>\n",
        "Brief summary of what was accomplished.\n",
        "</done>\n",
        "\n",
        "## Rules\n",
        "1. Execute ONE command at a time, then wait for the result.\n",
        "2. Parse API responses carefully - extract IDs and data needed for subsequent calls.\n",
        "3. If a command fails, analyze the error and try a different approach.\n",
        "4. Only use <done> when the task is fully completed (not just when you've gathered information).\n",
        "\n",
        "## API Documentation\n",
        "{api_docs}\n",
        "\"\"\"\n",
        "\n",
        "REACT_SYSTEM_PROMPT = \"\"\"You are an AI assistant that completes tasks by interacting with APIs via bash commands.\n",
        "\n",
        "## Current Session\n",
        "- **Service**: {service_name}\n",
        "- **Base URL**: {base_url}\n",
        "- **Description**: {service_description}\n",
        "\n",
        "## Environment\n",
        "- You are authenticated as a user in the {service_name} workspace/account.\n",
        "- Authentication is handled automatically via proxy. Use placeholder tokens like `<TOKEN>` where credentials would go.\n",
        "- You execute bash commands (primarily curl) to interact with the {service_name} API.\n",
        "- If you are not sure how to use {service_name} API, explore the endpoint, parameters, and learn how it works.\n",
        "- The environment is stateless between commands - you cannot install packages or persist files.\n",
        "\n",
        "## Response Format\n",
        "You must respond using XML tags. Think step-by-step, then execute a command OR declare completion.\n",
        "\n",
        "**To execute a bash command:**\n",
        "<thinking>\n",
        "Your reasoning about what needs to be done and why this command will help.\n",
        "</thinking>\n",
        "\n",
        "<action>\n",
        "Your bash command here (e.g., curl request)\n",
        "</action>\n",
        "\n",
        "**When the task is complete:**\n",
        "<thinking>\n",
        "Your reasoning confirming the task is done based on API responses.\n",
        "</thinking>\n",
        "\n",
        "<done>\n",
        "Brief summary of what was accomplished.\n",
        "</done>\n",
        "\n",
        "## Rules\n",
        "1. Execute ONE command at a time, then wait for the result.\n",
        "2. Parse API responses carefully - extract IDs and data needed for subsequent calls.\n",
        "3. If a command fails, analyze the error and try a different approach.\n",
        "4. Only use <done> when the task is fully completed (not just when you've gathered information).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Function to build the full system prompt\n",
        "def build_system_prompt(service: str, docs_markdown: str, include_api_docs: bool = True) -> str:\n",
        "    \"\"\"Build system prompt with service-specific context.\n",
        "    \n",
        "    Args:\n",
        "        service: Service name (slack, box, calendar, linear)\n",
        "        docs_markdown: Formatted API documentation markdown\n",
        "        include_api_docs: Whether to include API docs in the prompt\n",
        "    \n",
        "    Returns:\n",
        "        str: Complete system prompt\n",
        "    \"\"\"\n",
        "    config = SERVICE_CONFIG.get(service.lower(), {\n",
        "        \"name\": service,\n",
        "        \"base_url\": \"unknown\",\n",
        "        \"description\": f\"{service} API\",\n",
        "    })\n",
        "    \n",
        "    if include_api_docs:\n",
        "        return REACT_SYSTEM_PROMPT_WITH_API_DOCS.format(\n",
        "            service_name=config[\"name\"],\n",
        "            base_url=config[\"base_url\"],\n",
        "            service_description=config[\"description\"],\n",
        "            api_docs=docs_markdown,\n",
        "        )\n",
        "    else:\n",
        "        return REACT_SYSTEM_PROMPT.format(\n",
        "            service_name=config[\"name\"],\n",
        "            base_url=config[\"base_url\"],\n",
        "            service_description=config[\"description\"],\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "def parse_react_response(response: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:\n",
        "    \"\"\"\n",
        "    Parse ReAct XML response.\n",
        "    Returns: (thinking, action, done)\n",
        "    - If action is present: execute the command\n",
        "    - If done is present: task is complete\n",
        "    \"\"\"\n",
        "    thinking_match = re.search(r'<thinking>(.*?)</thinking>', response, re.DOTALL)\n",
        "    action_match = re.search(r'<action>(.*?)</action>', response, re.DOTALL)\n",
        "    done_match = re.search(r'<done>(.*?)</done>', response, re.DOTALL)\n",
        "    \n",
        "    thinking = thinking_match.group(1).strip() if thinking_match else None\n",
        "    action = action_match.group(1).strip() if action_match else None\n",
        "    done = done_match.group(1).strip() if done_match else None\n",
        "    \n",
        "    return thinking, action, done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25917f7e"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import time\n",
        "import httpx\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Any, List, Dict\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from agent_diff import (\n",
        "    AgentDiff,\n",
        "    BashExecutorProxy,\n",
        ")\n",
        "\n",
        "# ============ Benchmark Configurations ============\n",
        "\n",
        "BENCHMARK_CONFIGS = {\n",
        "    \"slack\": {\n",
        "        \"test_suite_name\": \"Slack Bench v2\",\n",
        "        \"docs_markdown\": slack_docs_markdown,\n",
        "    },\n",
        "    \"box\": {\n",
        "        \"test_suite_name\": \"Box Bench v2\",\n",
        "        \"docs_markdown\": box_docs_markdown,\n",
        "    },\n",
        "    \"calendar\": {\n",
        "        \"test_suite_name\": \"Calendar Bench\",\n",
        "        \"docs_markdown\": calendar_docs_markdown,\n",
        "    },\n",
        "    \"linear\": {\n",
        "        \"test_suite_name\": \"Linear Bench\",\n",
        "        \"docs_markdown\": linear_docs_markdown,\n",
        "    },\n",
        "}\n",
        "\n",
        "def get_benchmark_config(service: str, include_api_docs: bool = True) -> dict:\n",
        "    \"\"\"Get benchmark configuration for a service.\n",
        "    \n",
        "    Args:\n",
        "        service: Service name\n",
        "        include_api_docs: Whether to include API docs in the system prompt\n",
        "    \"\"\"\n",
        "    service_lower = service.lower()\n",
        "    if service_lower not in BENCHMARK_CONFIGS:\n",
        "        raise ValueError(f\"Unknown service: {service}. Available: {list(BENCHMARK_CONFIGS.keys())}\")\n",
        "    \n",
        "    config = BENCHMARK_CONFIGS[service_lower]\n",
        "    return {\n",
        "        \"service\": service_lower,\n",
        "        \"test_suite_name\": config[\"test_suite_name\"],\n",
        "        \"docs_markdown\": config[\"docs_markdown\"],\n",
        "        \"include_api_docs\": include_api_docs,\n",
        "        \"system_prompt\": build_system_prompt(service_lower, config[\"docs_markdown\"], include_api_docs),\n",
        "    }\n",
        "\n",
        "# ============ Output Directory ============\n",
        "\n",
        "OUTPUT_DIR = Path(\"evaluation_outputs\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# ============  ReAct Agent ============\n",
        "\n",
        "def call_openrouter(\n",
        "    model: str,\n",
        "    messages: List[Dict],\n",
        "    api_key: str,\n",
        ") -> Dict:\n",
        "    \"\"\"Make a completion request to OpenRouter API (no tool calling).\n",
        "    \n",
        "    Returns:\n",
        "        dict: {\n",
        "            \"content\": str,           # Model response text\n",
        "            \"finish_reason\": str,     # \"stop\", \"length\", etc.\n",
        "            \"usage\": {\n",
        "                \"prompt_tokens\": int,\n",
        "                \"completion_tokens\": int,\n",
        "                \"total_tokens\": int,\n",
        "                \"cost\": float,        # USD cost\n",
        "            }\n",
        "        }\n",
        "    \"\"\"\n",
        "    with httpx.Client(timeout=120) as client:\n",
        "        response = client.post(\n",
        "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
        "            headers={\n",
        "                \"Authorization\": f\"Bearer {api_key}\",\n",
        "                \"Content-Type\": \"application/json\",\n",
        "            },\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"messages\": messages,\n",
        "            },\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        \n",
        "        choice = data[\"choices\"][0]\n",
        "        usage = data.get(\"usage\", {})\n",
        "        \n",
        "        return {\n",
        "            \"content\": choice[\"message\"][\"content\"],\n",
        "            \"finish_reason\": choice.get(\"finish_reason\"),\n",
        "            \"usage\": {\n",
        "                \"prompt_tokens\": usage.get(\"prompt_tokens\", 0),\n",
        "                \"completion_tokens\": usage.get(\"completion_tokens\", 0),\n",
        "                \"total_tokens\": usage.get(\"total_tokens\", 0),\n",
        "                \"cost\": usage.get(\"cost\", 0.0),\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "def run_react_agent(\n",
        "    model_name: str,\n",
        "    task_prompt: str,\n",
        "    bash_executor: BashExecutorProxy,\n",
        "    system_prompt: str,\n",
        "    max_iterations: int = 25,\n",
        "    trace_accumulator: Dict = None,\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Custom ReAct agent loop using XML tags.\n",
        "    Returns structured trace with each step containing thinking, action, observation,\n",
        "    plus token usage and finish reasons.\n",
        "    \n",
        "    If trace_accumulator is provided, steps are written to it in real-time,\n",
        "    allowing partial trace recovery on timeout.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Task: {task_prompt}\"},\n",
        "    ]\n",
        "    \n",
        "    # Use provided accumulator or create new one\n",
        "    if trace_accumulator is not None:\n",
        "        steps = trace_accumulator.setdefault(\"steps\", [])\n",
        "        trace_accumulator[\"final\"] = None\n",
        "        trace_accumulator[\"completed\"] = False\n",
        "        trace_accumulator[\"usage\"] = {\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0, \"cost\": 0.0}\n",
        "    else:\n",
        "        steps = []\n",
        "    \n",
        "    final_step = None\n",
        "    completed = False\n",
        "    \n",
        "    # Track total usage across all iterations\n",
        "    total_usage = {\n",
        "        \"prompt_tokens\": 0,\n",
        "        \"completion_tokens\": 0,\n",
        "        \"total_tokens\": 0,\n",
        "        \"cost\": 0.0,\n",
        "    }\n",
        "    \n",
        "    for iteration in range(max_iterations):\n",
        "        # Get model response\n",
        "        try:\n",
        "            api_response = call_openrouter(\n",
        "                model=model_name,\n",
        "                messages=messages,\n",
        "                api_key=OPENAI_API_KEY,\n",
        "            )\n",
        "            response_text = api_response[\"content\"]\n",
        "            finish_reason = api_response[\"finish_reason\"]\n",
        "            iter_usage = api_response[\"usage\"]\n",
        "            \n",
        "            # Accumulate total usage\n",
        "            total_usage[\"prompt_tokens\"] += iter_usage[\"prompt_tokens\"]\n",
        "            total_usage[\"completion_tokens\"] += iter_usage[\"completion_tokens\"]\n",
        "            total_usage[\"total_tokens\"] += iter_usage[\"total_tokens\"]\n",
        "            total_usage[\"cost\"] += iter_usage[\"cost\"]\n",
        "            \n",
        "            # Update accumulator in real-time\n",
        "            if trace_accumulator is not None:\n",
        "                trace_accumulator[\"usage\"] = total_usage.copy()\n",
        "                \n",
        "        except Exception as e:\n",
        "            steps.append({\n",
        "                \"iteration\": iteration + 1,\n",
        "                \"error\": f\"API error: {str(e)}\",\n",
        "            })\n",
        "            break\n",
        "        \n",
        "        # Parse XML response\n",
        "        thinking, action, done = parse_react_response(response_text)\n",
        "        \n",
        "        # If model includes both, execute the action and ignore premature done\n",
        "        if action:\n",
        "            # Execute bash command\n",
        "            try:\n",
        "                result = bash_executor.execute(action)\n",
        "                # Normalize result to dict for consistent storage\n",
        "                if isinstance(result, dict):\n",
        "                    observation = {\n",
        "                        \"stdout\": result.get(\"stdout\", \"\"),\n",
        "                        \"stderr\": result.get(\"stderr\", \"\"),\n",
        "                        \"exit_code\": result.get(\"exit_code\", 0),\n",
        "                    }\n",
        "                else:\n",
        "                    observation = {\n",
        "                        \"stdout\": str(result) if result else \"\",\n",
        "                        \"stderr\": \"\",\n",
        "                        \"exit_code\": 0,\n",
        "                    }\n",
        "            except Exception as e:\n",
        "                observation = {\n",
        "                    \"stdout\": \"\",\n",
        "                    \"stderr\": str(e),\n",
        "                    \"exit_code\": 1,\n",
        "                    \"error\": str(e),\n",
        "                }\n",
        "            \n",
        "            # Record this step with nested structure + usage\n",
        "            steps.append({\n",
        "                \"iteration\": iteration + 1,\n",
        "                \"thinking\": thinking,\n",
        "                \"action\": action,\n",
        "                \"observation\": observation,\n",
        "                \"raw_response\": response_text,\n",
        "                \"finish_reason\": finish_reason,\n",
        "                \"usage\": iter_usage,\n",
        "            })\n",
        "            \n",
        "            # Format observation for model (just stdout, or error info)\n",
        "            if observation.get(\"exit_code\", 0) != 0:\n",
        "                obs_text = f\"{observation['stdout']}\\n[stderr]: {observation['stderr']}\\n[exit_code]: {observation['exit_code']}\".strip()\n",
        "            else:\n",
        "                obs_text = observation[\"stdout\"].strip() if observation[\"stdout\"] else \"(empty output)\"\n",
        "            \n",
        "            # Add to conversation\n",
        "            messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"<observation>\\n{obs_text}\\n</observation>\"})\n",
        "        elif done:\n",
        "            # Task completed (only when NO action present)\n",
        "            final_step = {\n",
        "                \"iteration\": iteration + 1,\n",
        "                \"thinking\": thinking,\n",
        "                \"summary\": done,\n",
        "                \"raw_response\": response_text,\n",
        "                \"finish_reason\": finish_reason,\n",
        "                \"usage\": iter_usage,\n",
        "            }\n",
        "            completed = True\n",
        "            break\n",
        "        else:\n",
        "            # No action and no done - malformed response\n",
        "            steps.append({\n",
        "                \"iteration\": iteration + 1,\n",
        "                \"thinking\": thinking,\n",
        "                \"warning\": \"No <action> or <done> tag found\",\n",
        "                \"raw_response\": response_text,\n",
        "                \"finish_reason\": finish_reason,\n",
        "                \"usage\": iter_usage,\n",
        "            })\n",
        "            messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "            messages.append({\"role\": \"user\", \"content\": \"Please respond with either an <action> to execute or <done> if the task is complete.\"})\n",
        "    \n",
        "    result = {\n",
        "        \"steps\": steps,\n",
        "        \"final\": final_step,\n",
        "        \"iterations\": iteration + 1,\n",
        "        \"completed\": completed,\n",
        "        \"usage\": total_usage,\n",
        "    }\n",
        "    \n",
        "    # Update accumulator if provided (for timeout recovery)\n",
        "    if trace_accumulator is not None:\n",
        "        trace_accumulator[\"final\"] = final_step\n",
        "        trace_accumulator[\"iterations\"] = iteration + 1\n",
        "        trace_accumulator[\"completed\"] = completed\n",
        "        trace_accumulator[\"usage\"] = total_usage\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "async def run_single_test(\n",
        "    client: AgentDiff, \n",
        "    model_name: str, \n",
        "    test: Any, \n",
        "    system_prompt: str,\n",
        "    test_timeout_seconds: int = 300,\n",
        "    max_iterations: int = 25,\n",
        ") -> tuple:\n",
        "    \"\"\"Run a single test case using custom ReAct agent.\n",
        "    \n",
        "    Args:\n",
        "        client: AgentDiff client instance\n",
        "        model_name: Model identifier (e.g., 'openai/gpt-5-mini')\n",
        "        test: Test object with id and prompt attributes\n",
        "        system_prompt: Full system prompt including API docs\n",
        "        test_timeout_seconds: Max seconds before timeout\n",
        "        max_iterations: Max ReAct loop iterations\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (test_id, result_dict) where result_dict contains:\n",
        "            - prompt (str): Task prompt\n",
        "            - status (str): 'passed', 'failed', 'timeout', or 'error'\n",
        "            - passed (bool): Whether assertions passed\n",
        "            - score (float): Score 0-100\n",
        "            - time (float): Execution seconds\n",
        "            - failures (list[str]): Failure messages\n",
        "            - runId (str): Run UUID\n",
        "            - error (str|None): Error message if status='error'\n",
        "            - trace (dict): Execution trace containing:\n",
        "                - steps (list): Each step has iteration, thinking, action, \n",
        "                  observation, raw_response, finish_reason, usage\n",
        "                - final (dict|None): Completion step with usage\n",
        "                - iterations (int): Total iterations\n",
        "                - completed (bool): Whether agent declared done\n",
        "                - usage (dict): Total {prompt_tokens, completion_tokens, \n",
        "                  total_tokens, cost}\n",
        "            - diff (dict|None): State changes {inserts, updates, deletes}\n",
        "    \"\"\"\n",
        "    test_id = test.id\n",
        "    prompt = test.prompt\n",
        "    response = None\n",
        "    timed_out = False\n",
        "    env = None\n",
        "\n",
        "    try:\n",
        "        # Initialize environment\n",
        "        env = client.init_env(testId=test_id)\n",
        "        run = client.start_run(envId=env.environmentId, testId=test_id)\n",
        "\n",
        "        # Create bash executor (direct, not LangChain tool)\n",
        "        bash_executor = BashExecutorProxy(\n",
        "            env.environmentId,\n",
        "            base_url=client.base_url,\n",
        "            api_key=client.api_key,\n",
        "        )\n",
        "\n",
        "        # Execution with timeout\n",
        "        # Use trace_accumulator to capture partial trace on timeout\n",
        "        trace_accumulator = {\n",
        "            \"steps\": [], \n",
        "            \"final\": None, \n",
        "            \"completed\": False,\n",
        "            \"usage\": {\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0, \"cost\": 0.0},\n",
        "        }\n",
        "        \n",
        "        start = time.perf_counter()\n",
        "        try:\n",
        "            response = await asyncio.wait_for(\n",
        "                asyncio.to_thread(\n",
        "                    run_react_agent,\n",
        "                    model_name=model_name,\n",
        "                    task_prompt=prompt,\n",
        "                    bash_executor=bash_executor,\n",
        "                    system_prompt=system_prompt,\n",
        "                    max_iterations=max_iterations,\n",
        "                    trace_accumulator=trace_accumulator,\n",
        "                ),\n",
        "                timeout=test_timeout_seconds\n",
        "            )\n",
        "        except asyncio.TimeoutError:\n",
        "            timed_out = True\n",
        "            # Use accumulated trace (partial) instead of losing it\n",
        "            response = {\n",
        "                \"steps\": trace_accumulator.get(\"steps\", []),\n",
        "                \"final\": trace_accumulator.get(\"final\"),\n",
        "                \"iterations\": len(trace_accumulator.get(\"steps\", [])),\n",
        "                \"completed\": False,\n",
        "                \"usage\": trace_accumulator.get(\"usage\", {}),\n",
        "                \"timeout_error\": f\"Test timed out after {test_timeout_seconds} seconds\",\n",
        "            }\n",
        "        except Exception as e:\n",
        "            response = {\n",
        "                \"steps\": trace_accumulator.get(\"steps\", []),\n",
        "                \"final\": trace_accumulator.get(\"final\"),\n",
        "                \"iterations\": len(trace_accumulator.get(\"steps\", [])),\n",
        "                \"completed\": False,\n",
        "                \"usage\": trace_accumulator.get(\"usage\", {}),\n",
        "                \"error\": str(e),\n",
        "            }\n",
        "        finally:\n",
        "            execution_time = time.perf_counter() - start\n",
        "\n",
        "        # Evaluation\n",
        "        score = client.evaluate_run(runId=run.runId)\n",
        "        run_result = client.get_results_for_run(runId=run.runId)\n",
        "\n",
        "        result = {\n",
        "            \"prompt\": prompt,\n",
        "            \"status\": \"timeout\" if timed_out else run_result.status,\n",
        "            \"passed\": False if timed_out else run_result.passed,\n",
        "            \"score\": 0 if timed_out else run_result.score.get(\"percent\", 0),\n",
        "            \"time\": round(execution_time, 2),\n",
        "            \"failures\": [\"Test timed out\"] if timed_out else run_result.failures,\n",
        "            \"runId\": run.runId,\n",
        "            \"trace\": response,\n",
        "            \"diff\": getattr(run_result, \"diff\", None),\n",
        "        }\n",
        "\n",
        "        # Cleanup\n",
        "        client.delete_env(envId=env.environmentId)\n",
        "        return test_id, result\n",
        "\n",
        "    except Exception as e:\n",
        "        # Cleanup on error if environment was created\n",
        "        if env:\n",
        "            try:\n",
        "                client.delete_env(envId=env.environmentId)\n",
        "            except:\n",
        "                pass\n",
        "        return test_id, {\"passed\": False, \"score\": 0, \"status\": \"error\", \"error\": str(e)}\n",
        "\n",
        "\n",
        "async def run_benchmark_suite(\n",
        "    service: str,\n",
        "    models: list,\n",
        "    runs_per_test: int = 1,\n",
        "    max_tests: int = None,\n",
        "    max_concurrent_models: int = 1,\n",
        "    max_concurrent_tests: int = 10,\n",
        "    max_calls_per_minute: int = 90,\n",
        "    test_timeout_seconds: int = 300,\n",
        "    max_iterations: int = 25,\n",
        "    include_api_docs: bool = True,\n",
        "):\n",
        "    \"\"\"Run benchmark for a single service.\n",
        "    \n",
        "    Args:\n",
        "        service: Service to benchmark ('slack', 'box', 'calendar', 'linear')\n",
        "        models: List of model identifiers to evaluate\n",
        "        runs_per_test: Number of times to run each test\n",
        "        max_tests: Maximum number of tests to run (None = all)\n",
        "        max_concurrent_models: Max parallel model evaluations\n",
        "        max_concurrent_tests: Max parallel test executions\n",
        "        max_calls_per_minute: Rate limit for API calls\n",
        "        test_timeout_seconds: Timeout per test in seconds\n",
        "        max_iterations: Max ReAct iterations per test\n",
        "        include_api_docs: Whether to include API documentation in system prompt\n",
        "    \n",
        "    Returns:\n",
        "        List[dict]: List of result dicts, each containing:\n",
        "            - prompt (str): The task prompt\n",
        "            - status (str): 'passed', 'failed', 'timeout', or 'error'\n",
        "            - passed (bool): Whether the test passed\n",
        "            - score (float): Score percentage (0-100)\n",
        "            - time (float): Execution time in seconds\n",
        "            - failures (list): List of failure messages\n",
        "            - runId (str): Unique run identifier\n",
        "            - error (str|None): Error message if status='error'\n",
        "            - model (str): Model identifier used\n",
        "            - test_id (str): Test case identifier (constant across runs)\n",
        "            - service (str): Service name (e.g., 'slack', 'box')\n",
        "            - test_suite_name (str): Full test suite name (e.g., 'Slack Bench v2')\n",
        "            - include_api_docs (bool): Whether API docs were included in prompt\n",
        "            - timestamp (str): ISO format timestamp when test was run\n",
        "            - trace (dict): Execution trace containing:\n",
        "                - steps (list): List of ReAct steps, each with:\n",
        "                    - iteration (int)\n",
        "                    - thinking (str): Model's reasoning\n",
        "                    - action (str): Bash command executed\n",
        "                    - observation (dict): {stdout, stderr, exit_code}\n",
        "                    - raw_response (str): Full model response\n",
        "                    - finish_reason (str): \"stop\", \"length\" (context overflow), etc.\n",
        "                    - usage (dict): {prompt_tokens, completion_tokens, total_tokens, cost}\n",
        "                - final (dict|None): Completion step with thinking, summary, usage\n",
        "                - iterations (int): Total iterations\n",
        "                - completed (bool): Whether agent declared done\n",
        "                - usage (dict): Total tokens/cost for entire run:\n",
        "                    {prompt_tokens, completion_tokens, total_tokens, cost}\n",
        "            - diff (dict|None): State diff with inserts, updates, deletes\n",
        "    \"\"\"\n",
        "    # Get benchmark configuration for this service\n",
        "    config = get_benchmark_config(service, include_api_docs=include_api_docs)\n",
        "    test_suite_name = config[\"test_suite_name\"]\n",
        "    system_prompt = config[\"system_prompt\"]\n",
        "    run_timestamp = datetime.now().isoformat()\n",
        "    \n",
        "    client = AgentDiff(\n",
        "        #api_key=AGENT_DIFF_API_KEY,\n",
        "        #base_url=AGENT_DIFF_BASE_URL,\n",
        "    )\n",
        "    try:\n",
        "        suite_list = client.list_test_suites(name=test_suite_name)\n",
        "        if not suite_list.testSuites:\n",
        "            print(f\"[ERROR] Test suite '{test_suite_name}' not found on AgentDiff server.\")\n",
        "            return []\n",
        "\n",
        "        suite = client.get_test_suite(suite_list.testSuites[0].id, expand=True)\n",
        "        tests = suite.tests[:max_tests] if max_tests else suite.tests\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Error connecting to AgentDiff: {e}\")\n",
        "        return []\n",
        "\n",
        "    total_logical = len(tests) * len(models)\n",
        "    total_runs = total_logical * runs_per_test\n",
        "    print(f\"\\n[{config['service'].upper()}] {test_suite_name} | {len(tests)} tests x {len(models)} models x {runs_per_test} runs = {total_runs} total\")\n",
        "\n",
        "    semaphore = asyncio.Semaphore(max_concurrent_models * max_concurrent_tests)\n",
        "\n",
        "    # rate limiting state (per minute window)\n",
        "    window_seconds = 60\n",
        "    window_start = time.monotonic()\n",
        "    calls_in_window = 0\n",
        "    rate_lock = asyncio.Lock()\n",
        "\n",
        "    async def acquire_rate_slot():\n",
        "        nonlocal window_start, calls_in_window\n",
        "        while True:\n",
        "            async with rate_lock:\n",
        "                now = time.monotonic()\n",
        "                # reset window if needed\n",
        "                if now - window_start >= window_seconds:\n",
        "                    window_start = now\n",
        "                    calls_in_window = 0\n",
        "\n",
        "                if calls_in_window < max_calls_per_minute:\n",
        "                    calls_in_window += 1\n",
        "                    return  # allowed to proceed\n",
        "\n",
        "                # need to wait until current window ends\n",
        "                sleep_for = window_seconds - (now - window_start)\n",
        "            # sleep outside the lock\n",
        "            if sleep_for > 0:\n",
        "                await asyncio.sleep(sleep_for)\n",
        "\n",
        "    # Progress tracking state\n",
        "    completed_results = []\n",
        "    results_lock = asyncio.Lock()\n",
        "    \n",
        "    # Create progress bar\n",
        "    pbar = tqdm(\n",
        "        total=total_runs,\n",
        "        desc=f\"{config['service'].upper()} | 0/{total_runs} | 0.0% pass | 0.0% avg\",\n",
        "        unit=\"test\",\n",
        "        leave=True,\n",
        "        dynamic_ncols=True,\n",
        "        mininterval=0.1,\n",
        "    )\n",
        "    pbar.refresh()  # Force initial display\n",
        "    \n",
        "    async def update_progress():\n",
        "        \"\"\"Update progress bar with current stats.\"\"\"\n",
        "        async with results_lock:\n",
        "            n = len(completed_results)\n",
        "            if n > 0:\n",
        "                passed = sum(1 for r in completed_results if r.get(\"passed\"))\n",
        "                avg_score = sum(r.get(\"score\", 0) for r in completed_results) / n\n",
        "                pass_rate = 100 * passed / n\n",
        "                pbar.set_description(\n",
        "                    f\"{config['service'].upper()} | {passed}/{n} | {pass_rate:.1f}% pass | {avg_score:.1f}% avg\"\n",
        "                )\n",
        "\n",
        "    async def worker(model_name, test):\n",
        "        await acquire_rate_slot()\n",
        "        async with semaphore:\n",
        "            tid, res = await run_single_test(\n",
        "                client, model_name, test, system_prompt,\n",
        "                test_timeout_seconds=test_timeout_seconds,\n",
        "                max_iterations=max_iterations,\n",
        "            )\n",
        "            res[\"model\"] = model_name\n",
        "            res[\"test_id\"] = tid\n",
        "            \n",
        "            # Track result and update progress\n",
        "            async with results_lock:\n",
        "                completed_results.append(res)\n",
        "            \n",
        "            pbar.update(1)\n",
        "            await update_progress()\n",
        "            \n",
        "            # Log failures to tqdm (won't mess up progress bar)\n",
        "            if not res.get(\"passed\"):\n",
        "                if res.get(\"status\") == \"timeout\":\n",
        "                    tqdm.write(f\"[TIMEOUT] {tid[:8]}... | {res.get('time', 0):.1f}s\")\n",
        "                elif res.get(\"status\") == \"error\":\n",
        "                    tqdm.write(f\"[ERROR] {tid[:8]}... | {res.get('error', 'unknown')}\")\n",
        "                else:\n",
        "                    tqdm.write(f\"[FAIL] {tid[:8]}... | {res.get('score')}% | {(res.get('failures') or ['unknown'])[:1]}\")\n",
        "            \n",
        "            return res\n",
        "\n",
        "    tasks = []\n",
        "    for model in models:\n",
        "        for test in tests:\n",
        "            for _ in range(runs_per_test):\n",
        "                tasks.append(worker(model, test))\n",
        "\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    pbar.close()\n",
        "    \n",
        "    # Add metadata to each result\n",
        "    for r in results:\n",
        "        r[\"service\"] = config[\"service\"]\n",
        "        r[\"test_suite_name\"] = test_suite_name\n",
        "        r[\"include_api_docs\"] = include_api_docs\n",
        "        r[\"timestamp\"] = run_timestamp\n",
        "    \n",
        "    # Final summary\n",
        "    passed = sum(1 for r in results if r.get(\"passed\"))\n",
        "    avg_score = sum(r.get(\"score\", 0) for r in results) / len(results) if results else 0\n",
        "    print(f\"{config['service'].upper()} Complete: {passed}/{len(results)} passed ({avg_score:.1f}% avg)\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "async def run_all_benchmarks(\n",
        "    models: list,\n",
        "    services: list = None,\n",
        "    runs_per_test: int = 1,\n",
        "    max_tests: int = None,\n",
        "    max_concurrent_models: int = 1,\n",
        "    max_concurrent_tests: int = 10,\n",
        "    max_calls_per_minute: int = 90,\n",
        "    test_timeout_seconds: int = 300,\n",
        "    max_iterations: int = 25,\n",
        "    include_api_docs: bool = True,\n",
        "):\n",
        "    \"\"\"Run benchmarks for multiple services.\n",
        "    \n",
        "    Args:\n",
        "        models: List of model identifiers to evaluate\n",
        "        services: List of services to benchmark. If None, runs all available services.\n",
        "                  Options: ['slack', 'box', 'calendar', 'linear']\n",
        "        runs_per_test: Number of times to run each test\n",
        "        max_tests: Maximum number of tests to run per service (None = all)\n",
        "        max_concurrent_models: Max parallel model evaluations\n",
        "        max_concurrent_tests: Max parallel test executions\n",
        "        max_calls_per_minute: Rate limit for API calls\n",
        "        test_timeout_seconds: Timeout per test in seconds\n",
        "        max_iterations: Max ReAct iterations per test\n",
        "        include_api_docs: Whether to include API documentation in system prompt\n",
        "    \n",
        "    Returns:\n",
        "        Dict[str, List[dict]]: Mapping of service name to list of results.\n",
        "            Each result dict contains (see run_benchmark_suite for full schema):\n",
        "            - prompt, status, passed, score, time, failures, error\n",
        "            - runId, model, test_id, service, test_suite_name\n",
        "            - include_api_docs (bool), timestamp (ISO format)\n",
        "            - trace: {steps, final, iterations, completed, usage}\n",
        "              - Each step includes: finish_reason, usage (per-iteration tokens/cost)\n",
        "              - usage: Total {prompt_tokens, completion_tokens, total_tokens, cost}\n",
        "            - diff: {inserts, updates, deletes}\n",
        "    \"\"\"\n",
        "    if services is None:\n",
        "        services = list(BENCHMARK_CONFIGS.keys())\n",
        "    \n",
        "    docs_status = \"with API docs\" if include_api_docs else \"NO API docs\"\n",
        "    print(f\"Benchmarks: {', '.join(s.upper() for s in services)} | {len(models)} models | {docs_status} | {test_timeout_seconds}s timeout\")\n",
        "    \n",
        "    all_results = {}\n",
        "    for service in services:\n",
        "        try:\n",
        "            results = await run_benchmark_suite(\n",
        "                service=service,\n",
        "                models=models,\n",
        "                runs_per_test=runs_per_test,\n",
        "                max_tests=max_tests,\n",
        "                max_concurrent_models=max_concurrent_models,\n",
        "                max_concurrent_tests=max_concurrent_tests,\n",
        "                max_calls_per_minute=max_calls_per_minute,\n",
        "                test_timeout_seconds=test_timeout_seconds,\n",
        "                max_iterations=max_iterations,\n",
        "                include_api_docs=include_api_docs,\n",
        "            )\n",
        "            all_results[service] = results\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Error running {service} benchmark: {e}\")\n",
        "            all_results[service] = []\n",
        "    \n",
        "    # Overall summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"OVERALL SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    total_passed = 0\n",
        "    total_tests = 0\n",
        "    for service, results in all_results.items():\n",
        "        if results:\n",
        "            passed = sum(1 for r in results if r.get(\"passed\"))\n",
        "            total = len(results)\n",
        "            total_passed += passed\n",
        "            total_tests += total\n",
        "            print(f\"  {service.upper()}: {passed}/{total} passed\")\n",
        "    \n",
        "    if total_tests > 0:\n",
        "        print(f\"\\n  TOTAL: {total_passed}/{total_tests} passed ({100*total_passed/total_tests:.1f}%)\")\n",
        "    \n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Models to evaluate (uncomment to include)\n",
        "\n",
        "MODELS = [\n",
        "    \"openai/gpt-5-mini\",\n",
        "    # \"anthropic/claude-haiku-4.5\",\n",
        "    # \"anthropic/claude-sonnet-4.5\",\n",
        "    # \"anthropic/claude-opus-4.5\",\n",
        "    # \"x-ai/grok-4.1-fast\",\n",
        "    #\"deepseek/deepseek-v3.2\",\n",
        "    # \"moonshotai/kimi-k2-0905\",\n",
        "    # \"qwen/qwen3-vl-235b-a22b-instruct\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "812e87c0",
        "outputId": "914dd2f4-aabf-4478-8fa5-8a1b9728ab58"
      },
      "outputs": [],
      "source": [
        "# Runtime Settings (all passed to benchmark functions)\n",
        "\n",
        "BENCHMARK_SETTINGS = {\n",
        "    \"models\": MODELS,                    # Models to evaluate (from cell above)\n",
        "    \"runs_per_test\": 2,                  # Number of runs per test\n",
        "    \"max_tests\": None,                      # None = all tests, or set a limit\n",
        "    \"max_concurrent_models\": 1,          # Parallel model evaluations\n",
        "    \"max_concurrent_tests\": 10,          # Parallel test executions\n",
        "    \"max_calls_per_minute\": 120,         # API rate limit\n",
        "    \"test_timeout_seconds\": 300,         # 5 minutes per test\n",
        "    \"max_iterations\": 25,                # Max ReAct iterations\n",
        "    \"include_api_docs\": True,            # Include API docs in prompt (False = agent explores)\n",
        "}\n",
        "\n",
        "# Run benchmark (uncomment one option):\n",
        "\n",
        "# Single service:\n",
        "#results = await run_benchmark_suite(service=\"slack\", **BENCHMARK_SETTINGS)\n",
        "\n",
        "# Multiple services:\n",
        "all_results = await run_all_benchmarks(services=[\"slack\", \"box\"], **BENCHMARK_SETTINGS)\n",
        "\n",
        "# All services:\n",
        "# all_results = await run_all_benchmarks(**BENCHMARK_SETTINGS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle both single service (results = list) and multi-service (all_results = dict)\n",
        "if 'all_results' in dir() and all_results:\n",
        "    # Flatten all_results dict into a single list\n",
        "    results_to_save = []\n",
        "    for service, service_results in all_results.items():\n",
        "        results_to_save.extend(service_results)\n",
        "elif 'results' in dir() and results:\n",
        "    results_to_save = results\n",
        "else:\n",
        "    results_to_save = []\n",
        "\n",
        "if results_to_save:\n",
        "    df = pd.DataFrame(results_to_save)\n",
        "\n",
        "    # Group by model to get leaderboard\n",
        "    leaderboard = df.groupby(\"model\").agg(\n",
        "        passed=(\"passed\", \"sum\"),\n",
        "        total=(\"passed\", \"count\"),\n",
        "        avg_score=(\"score\", \"mean\"),\n",
        "        avg_time=(\"time\", \"mean\")\n",
        "    ).reset_index()\n",
        "\n",
        "    leaderboard[\"pass_rate\"] = (leaderboard[\"passed\"] / leaderboard[\"total\"] * 100).round(1)\n",
        "    leaderboard = leaderboard.sort_values(\"pass_rate\", ascending=False)\n",
        "\n",
        "    display(HTML(\"<h3>Final Results</h3>\"))\n",
        "    display(leaderboard)\n",
        "    \n",
        "    total_cost = 0.0\n",
        "    total_tokens = 0\n",
        "    for r in results_to_save:\n",
        "        if \"trace\" in r and \"usage\" in r[\"trace\"]:\n",
        "            total_cost += r[\"trace\"][\"usage\"].get(\"cost\", 0)\n",
        "            total_tokens += r[\"trace\"][\"usage\"].get(\"total_tokens\", 0)\n",
        "    \n",
        "    if total_cost > 0 or total_tokens > 0:\n",
        "        print(f\"\\nUsage Summary: {total_tokens:,} tokens | ${total_cost:.4f} USD\")\n",
        "\n",
        "    # Save using standard json library to handle encoding errors robustly\n",
        "    ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    output_path = OUTPUT_DIR / f\"full_results_{ts}.json\"\n",
        "\n",
        "    def safe_serialize(obj):\n",
        "        if isinstance(obj, bytes):\n",
        "            return obj.decode('utf-8', errors='replace')\n",
        "        return str(obj)\n",
        "\n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results_to_save, f, indent=2, default=safe_serialize, ensure_ascii=False)\n",
        "        print(f\"Results saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving JSON: {e}\")\n",
        "else:\n",
        "    print(\"No results generated.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility: Merge Multiple Result Files\n",
        "\n",
        "def list_result_files(output_dir: Path = OUTPUT_DIR) -> list:\n",
        "    \"\"\"List all result JSON files in the output directory.\"\"\"\n",
        "    files = sorted(output_dir.glob(\"full_results_*.json\"))\n",
        "    print(f\"Found {len(files)} result files in {output_dir}:\")\n",
        "    for i, f in enumerate(files):\n",
        "        size_kb = f.stat().st_size / 1024\n",
        "        print(f\"  [{i}] {f.name} ({size_kb:.1f} KB)\")\n",
        "    return files\n",
        "\n",
        "\n",
        "def merge_result_files(\n",
        "    files: list = None,\n",
        "    output_dir: Path = OUTPUT_DIR,\n",
        "    output_name: str = None,\n",
        "    deduplicate: bool = False,\n",
        ") -> list:\n",
        "    \"\"\"Merge multiple result JSON files into one.\n",
        "    \n",
        "    Args:\n",
        "        files: List of file paths to merge. If None, merges all files in output_dir.\n",
        "        output_dir: Directory containing result files (used if files=None)\n",
        "        output_name: Output filename. If None, generates timestamped name.\n",
        "        deduplicate: If True, removes duplicate entries (same test_id + model + timestamp)\n",
        "    \n",
        "    Returns:\n",
        "        list: Merged results\n",
        "    \"\"\"\n",
        "    if files is None:\n",
        "        files = sorted(output_dir.glob(\"full_results_*.json\"))\n",
        "    \n",
        "    if not files:\n",
        "        print(\"No files to merge.\")\n",
        "        return []\n",
        "    \n",
        "    print(f\"Merging {len(files)} files...\")\n",
        "    \n",
        "    all_results = []\n",
        "    for f in files:\n",
        "        try:\n",
        "            with open(f, 'r', encoding='utf-8') as fp:\n",
        "                data = json.load(fp)\n",
        "                if isinstance(data, list):\n",
        "                    all_results.extend(data)\n",
        "                    print(f\"  + {f.name}: {len(data)} results\")\n",
        "                else:\n",
        "                    print(f\"  ! {f.name}: unexpected format (not a list)\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ! {f.name}: error loading - {e}\")\n",
        "    \n",
        "    print(f\"\\nTotal: {len(all_results)} results\")\n",
        "    \n",
        "    # Deduplicate if requested\n",
        "    if deduplicate and all_results:\n",
        "        seen = set()\n",
        "        unique_results = []\n",
        "        for r in all_results:\n",
        "            # Create unique key from test_id, model, and timestamp\n",
        "            key = (r.get(\"test_id\"), r.get(\"model\"), r.get(\"timestamp\"))\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                unique_results.append(r)\n",
        "        \n",
        "        removed = len(all_results) - len(unique_results)\n",
        "        if removed > 0:\n",
        "            print(f\"Deduplicated: removed {removed} duplicates, {len(unique_results)} unique results\")\n",
        "        all_results = unique_results\n",
        "    \n",
        "    # Save merged results\n",
        "    if output_name is None:\n",
        "        ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        output_name = f\"merged_results_{ts}.json\"\n",
        "    \n",
        "    output_path = output_dir / output_name\n",
        "    \n",
        "    def safe_serialize(obj):\n",
        "        if isinstance(obj, bytes):\n",
        "            return obj.decode('utf-8', errors='replace')\n",
        "        return str(obj)\n",
        "    \n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, indent=2, default=safe_serialize, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"\\nSaved to: {output_path}\")\n",
        "    \n",
        "    # Summary by model and service\n",
        "    if all_results:\n",
        "        df = pd.DataFrame(all_results)\n",
        "        print(\"\\n--- Summary by Model ---\")\n",
        "        model_summary = df.groupby(\"model\").agg(\n",
        "            tests=(\"passed\", \"count\"),\n",
        "            passed=(\"passed\", \"sum\"),\n",
        "            avg_score=(\"score\", \"mean\"),\n",
        "        ).reset_index()\n",
        "        model_summary[\"pass_rate\"] = (100 * model_summary[\"passed\"] / model_summary[\"tests\"]).round(1)\n",
        "        display(model_summary)\n",
        "        \n",
        "        if \"service\" in df.columns:\n",
        "            print(\"\\n--- Summary by Service ---\")\n",
        "            service_summary = df.groupby(\"service\").agg(\n",
        "                tests=(\"passed\", \"count\"),\n",
        "                passed=(\"passed\", \"sum\"),\n",
        "                avg_score=(\"score\", \"mean\"),\n",
        "            ).reset_index()\n",
        "            service_summary[\"pass_rate\"] = (100 * service_summary[\"passed\"] / service_summary[\"tests\"]).round(1)\n",
        "            display(service_summary)\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# files = list_result_files()\n",
        "# merged = merge_result_files()  # Merge all files\n",
        "# merged = merge_result_files(files=[files[0], files[2]])  # Merge specific files\n",
        "# merged = merge_result_files(deduplicate=True)  # Merge and remove duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List available result files\n",
        "files = list_result_files()\n",
        "\n",
        "# Merge all files with deduplication\n",
        "merged = merge_result_files(deduplicate=True)\n",
        "\n",
        "# Alternative options:\n",
        "# merged = merge_result_files()  # Merge all without deduplication\n",
        "# merged = merge_result_files(files=[files[0], files[-1]])  # Merge specific files only"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
