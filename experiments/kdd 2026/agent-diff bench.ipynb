{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip uninstall agent-diff -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install agent-diff langchain langchain-openai langchain-anthropic pandas matplotlib -q "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ca31907"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "#AGENT_DIFF_API_KEY = \"\"\n",
        "#AGENT_DIFF_BASE_URL = \"\"\n",
        "OPENAI_API_KEY = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Base path relative to this notebook\n",
        "DOCS_BASE = Path(\"../../examples\")\n",
        "\n",
        "def load_api_docs(filepath: Path) -> dict:\n",
        "    \"\"\"Load API docs JSON, return empty dict if not found.\"\"\"\n",
        "    if filepath.exists():\n",
        "        return json.load(open(filepath))\n",
        "    print(f\"Docs not found: {filepath}\")\n",
        "    return {}\n",
        "\n",
        "def format_docs_markdown(docs: dict) -> str:\n",
        "    \"\"\"Convert API docs dict to markdown format.\"\"\"\n",
        "    if not docs:\n",
        "        return \"\"\n",
        "    \n",
        "    markdown = \"\"\n",
        "    for endpoint, info in docs.items():\n",
        "        markdown += f\"## {endpoint}\\n\"\n",
        "        markdown += f\"{info.get('description', '')}\\n\\n\"\n",
        "        \n",
        "        if info.get('parameters'):\n",
        "            markdown += \"**Parameters:**\\n\"\n",
        "            for location, params in info['parameters'].items():\n",
        "                markdown += f\"  {location}:\\n\"\n",
        "                if not isinstance(params, dict):\n",
        "                    markdown += f\"    {params}\\n\"\n",
        "                    continue\n",
        "                for param_name, param_info in params.items():\n",
        "                    # Handle case where param_info might be a string\n",
        "                    if not isinstance(param_info, dict):\n",
        "                        markdown += f\"    - `{param_name}`: {param_info}\\n\"\n",
        "                        continue\n",
        "                    required = \"**required**\" if param_info.get('required') else \"optional\"\n",
        "                    param_type = param_info.get('type', 'any')\n",
        "                    param_desc = param_info.get('description', '')\n",
        "                    markdown += f\"    - `{param_name}` ({param_type}, {required}): {param_desc}\\n\"\n",
        "            markdown += \"\\n\"\n",
        "    \n",
        "    return markdown\n",
        "\n",
        "# Load available docs (all services)\n",
        "slack_docs = load_api_docs(DOCS_BASE / \"slack/testsuites/slack_docs/slack_api_full_docs.json\")\n",
        "box_docs = load_api_docs(DOCS_BASE / \"box/testsuites/box_docs/box_api_full_docs.json\")\n",
        "calendar_docs = load_api_docs(DOCS_BASE / \"calendar/testsuites/calendar_docs/calendar_api_full_docs.json\")\n",
        "linear_docs = load_api_docs(DOCS_BASE / \"linear/testsuites/linear_docs/linear_api_full_docs.json\")\n",
        "\n",
        "# Format to markdown\n",
        "slack_docs_markdown = format_docs_markdown(slack_docs)\n",
        "box_docs_markdown = format_docs_markdown(box_docs)\n",
        "calendar_docs_markdown = format_docs_markdown(calendar_docs)\n",
        "linear_docs_markdown = format_docs_markdown(linear_docs)\n",
        "\n",
        "# Summary\n",
        "print(f\"[{'OK' if slack_docs else 'MISSING'}] Slack docs: {len(slack_docs)} endpoints\")\n",
        "print(f\"[{'OK' if box_docs else 'MISSING'}] Box docs: {len(box_docs)} endpoints\")\n",
        "print(f\"[{'OK' if calendar_docs else 'MISSING'}] Calendar docs: {len(calendar_docs)} endpoints\")\n",
        "print(f\"[{'OK' if linear_docs else 'MISSING'}] Linear docs: {len(linear_docs)} endpoints\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "# Service configurations with base URLs\n",
        "SERVICE_CONFIG = {\n",
        "    \"slack\": {\n",
        "        \"name\": \"Slack\",\n",
        "        \"base_url\": \"https://slack.com/api\",\n",
        "        \"description\": \"Slack workspace messaging and collaboration API\",\n",
        "        \"extra_context\": \"\",\n",
        "    },\n",
        "    \"box\": {\n",
        "        \"name\": \"Box\",\n",
        "        \"base_url\": \"https://api.box.com/2.0\",\n",
        "        \"description\": \"Box cloud storage and file management API\",\n",
        "        \"extra_context\": \"\",\n",
        "    },\n",
        "    \"calendar\": {\n",
        "        \"name\": \"Google Calendar\",\n",
        "        \"base_url\": \"https://www.googleapis.com/calendar/v3\",\n",
        "        \"description\": \"Google Calendar scheduling and events API\",\n",
        "        \"extra_context\": \"- **Current Date/Time**: Sunday, June 17, 2018 at 00:01 (midnight), timezone America/Los_Angeles. Use this as the reference point for all relative date/time expressions like 'today', 'tomorrow', 'this Saturday', etc.\",\n",
        "    },\n",
        "    \"linear\": {\n",
        "        \"name\": \"Linear\",\n",
        "        \"base_url\": \"https://api.linear.app/graphql\",\n",
        "        \"description\": \"Linear project management and issue tracking API\",\n",
        "        \"extra_context\": \"\",\n",
        "    },\n",
        "}\n",
        "\n",
        "# ReAct System Prompt \n",
        "REACT_SYSTEM_PROMPT_WITH_API_DOCS = \"\"\"You are an AI assistant that completes tasks by interacting with APIs via bash commands.\n",
        "\n",
        "## Current Session\n",
        "- **Service**: {service_name}\n",
        "- **Base URL**: {base_url}\n",
        "- **Description**: {service_description}\n",
        "{extra_context}\n",
        "\n",
        "## Environment\n",
        "- You are authenticated as a user in the {service_name} workspace/account.\n",
        "- Authentication is handled automatically via proxy. Use placeholder tokens like `<TOKEN>` where credentials would go.\n",
        "- You execute bash commands (primarily curl) to interact with the {service_name} API.\n",
        "- The environment is stateless between commands - you cannot install packages or persist files.\n",
        "\n",
        "## Response Format\n",
        "You must respond using XML tags. Think step-by-step, then execute a command OR declare completion.\n",
        "\n",
        "**To execute a bash command:**\n",
        "<thinking>\n",
        "Your reasoning about what needs to be done and why this command will help.\n",
        "</thinking>\n",
        "\n",
        "<action>\n",
        "Your bash command here (e.g., curl request)\n",
        "</action>\n",
        "\n",
        "**When the task is complete:**\n",
        "<thinking>\n",
        "Your reasoning confirming the task is done based on API responses.\n",
        "</thinking>\n",
        "\n",
        "<done>\n",
        "Brief summary of what was accomplished.\n",
        "</done>\n",
        "\n",
        "## Rules\n",
        "1. Execute ONE command at a time, then wait for the result.\n",
        "2. Parse API responses carefully - extract IDs and data needed for subsequent calls.\n",
        "3. If a command fails, analyze the error and try a different approach.\n",
        "4. Only use <done> when the task is fully completed (not just when you've gathered information).\n",
        "\n",
        "## API Documentation\n",
        "{api_docs}\n",
        "\"\"\"\n",
        "\n",
        "REACT_SYSTEM_PROMPT = \"\"\"You are an AI assistant that completes tasks by interacting with APIs via bash commands.\n",
        "\n",
        "## Current Session\n",
        "- **Service**: {service_name}\n",
        "- **Base URL**: {base_url}\n",
        "- **Description**: {service_description}\n",
        "{extra_context}\n",
        "\n",
        "## Environment\n",
        "- You are authenticated as a user in the {service_name} workspace/account.\n",
        "- Authentication is handled automatically via proxy. Use placeholder tokens like `<TOKEN>` where credentials would go.\n",
        "- You execute bash commands (primarily curl) to interact with the {service_name} API.\n",
        "- If you are not sure how to use {service_name} API, explore the endpoint, parameters, and learn how it works.\n",
        "- The environment is stateless between commands - you cannot install packages or persist files.\n",
        "\n",
        "## Response Format\n",
        "You must respond using XML tags. Think step-by-step, then execute a command OR declare completion.\n",
        "\n",
        "**To execute a bash command:**\n",
        "<thinking>\n",
        "Your reasoning about what needs to be done and why this command will help.\n",
        "</thinking>\n",
        "\n",
        "<action>\n",
        "Your bash command here (e.g., curl request)\n",
        "</action>\n",
        "\n",
        "**When the task is complete:**\n",
        "<thinking>\n",
        "Your reasoning confirming the task is done based on API responses.\n",
        "</thinking>\n",
        "\n",
        "<done>\n",
        "Brief summary of what was accomplished.\n",
        "</done>\n",
        "\n",
        "## Rules\n",
        "1. Execute ONE command at a time, then wait for the result.\n",
        "2. Parse API responses carefully - extract IDs and data needed for subsequent calls.\n",
        "3. If a command fails, analyze the error and try a different approach.\n",
        "4. Only use <done> when the task is fully completed (not just when you've gathered information).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Function to build the full system prompt\n",
        "def build_system_prompt(service: str, docs_markdown: str, include_api_docs: bool = True) -> str:\n",
        "    \"\"\"Build system prompt with service-specific context.\n",
        "    \n",
        "    Args:\n",
        "        service: Service name (slack, box, calendar, linear)\n",
        "        docs_markdown: Formatted API documentation markdown\n",
        "        include_api_docs: Whether to include API docs in the prompt\n",
        "    \n",
        "    Returns:\n",
        "        str: Complete system prompt\n",
        "    \"\"\"\n",
        "    config = SERVICE_CONFIG.get(service.lower(), {\n",
        "        \"name\": service,\n",
        "        \"base_url\": \"unknown\",\n",
        "        \"description\": f\"{service} API\",\n",
        "        \"extra_context\": \"\",\n",
        "    })\n",
        "    \n",
        "    extra_context = config.get(\"extra_context\", \"\")\n",
        "    \n",
        "    if include_api_docs:\n",
        "        return REACT_SYSTEM_PROMPT_WITH_API_DOCS.format(\n",
        "            service_name=config[\"name\"],\n",
        "            base_url=config[\"base_url\"],\n",
        "            service_description=config[\"description\"],\n",
        "            extra_context=extra_context,\n",
        "            api_docs=docs_markdown,\n",
        "        )\n",
        "    else:\n",
        "        return REACT_SYSTEM_PROMPT.format(\n",
        "            service_name=config[\"name\"],\n",
        "            base_url=config[\"base_url\"],\n",
        "            service_description=config[\"description\"],\n",
        "            extra_context=extra_context,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "def parse_react_response(response: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:\n",
        "    \"\"\"\n",
        "    Parse ReAct XML response.\n",
        "    Returns: (thinking, action, done)\n",
        "    - If action is present: execute the command\n",
        "    - If done is present: task is complete\n",
        "    \"\"\"\n",
        "    thinking_match = re.search(r'<thinking>(.*?)</thinking>', response, re.DOTALL)\n",
        "    action_match = re.search(r'<action>(.*?)</action>', response, re.DOTALL)\n",
        "    done_match = re.search(r'<done>(.*?)</done>', response, re.DOTALL)\n",
        "    \n",
        "    thinking = thinking_match.group(1).strip() if thinking_match else None\n",
        "    action = action_match.group(1).strip() if action_match else None\n",
        "    done = done_match.group(1).strip() if done_match else None\n",
        "    \n",
        "    return thinking, action, done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25917f7e"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import time\n",
        "import httpx\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Any, List, Dict\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from agent_diff import (\n",
        "    AgentDiff,\n",
        "    BashExecutorProxy,\n",
        ")\n",
        "\n",
        "# ============ Benchmark Configurations ============\n",
        "\n",
        "BENCHMARK_CONFIGS = {\n",
        "    \"slack\": {\n",
        "        \"test_suite_name\": \"Slack Bench v2\",\n",
        "        \"docs_markdown\": slack_docs_markdown,\n",
        "    },\n",
        "    \"box\": {\n",
        "        \"test_suite_name\": \"Box Bench v2\",\n",
        "        \"docs_markdown\": box_docs_markdown,\n",
        "    },\n",
        "    \"calendar\": {\n",
        "        \"test_suite_name\": \"Calendar Bench\",\n",
        "        \"docs_markdown\": calendar_docs_markdown,\n",
        "    },\n",
        "    \"linear\": {\n",
        "        \"test_suite_name\": \"Linear Bench\",\n",
        "        \"docs_markdown\": linear_docs_markdown,\n",
        "    },\n",
        "}\n",
        "\n",
        "def get_benchmark_config(service: str, include_api_docs: bool = True) -> dict:\n",
        "    \"\"\"Get benchmark configuration for a service.\n",
        "    \n",
        "    Args:\n",
        "        service: Service name\n",
        "        include_api_docs: Whether to include API docs in the system prompt\n",
        "    \"\"\"\n",
        "    service_lower = service.lower()\n",
        "    if service_lower not in BENCHMARK_CONFIGS:\n",
        "        raise ValueError(f\"Unknown service: {service}. Available: {list(BENCHMARK_CONFIGS.keys())}\")\n",
        "    \n",
        "    config = BENCHMARK_CONFIGS[service_lower]\n",
        "    return {\n",
        "        \"service\": service_lower,\n",
        "        \"test_suite_name\": config[\"test_suite_name\"],\n",
        "        \"docs_markdown\": config[\"docs_markdown\"],\n",
        "        \"include_api_docs\": include_api_docs,\n",
        "        \"system_prompt\": build_system_prompt(service_lower, config[\"docs_markdown\"], include_api_docs),\n",
        "    }\n",
        "\n",
        "# ============ Output Directory ============\n",
        "\n",
        "OUTPUT_DIR = Path(\"evaluation_outputs\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# ============  ReAct Agent ============\n",
        "\n",
        "def call_openrouter(\n",
        "    model: str,\n",
        "    messages: List[Dict],\n",
        "    api_key: str,\n",
        "    max_retries: int = 3,\n",
        "    base_delay: float = 2.0,\n",
        ") -> Dict:\n",
        "    \"\"\"Make a completion request to OpenRouter API (no tool calling).\n",
        "    \n",
        "    Includes retry logic for transient failures (502, 503, connection errors).\n",
        "    \n",
        "    Returns:\n",
        "        dict: {\n",
        "            \"content\": str,           # Model response text\n",
        "            \"finish_reason\": str,     # \"stop\", \"length\", etc.\n",
        "            \"usage\": {\n",
        "                \"prompt_tokens\": int,\n",
        "                \"completion_tokens\": int,\n",
        "                \"total_tokens\": int,\n",
        "                \"cost\": float,        # USD cost\n",
        "            }\n",
        "        }\n",
        "    \"\"\"\n",
        "    import time\n",
        "    import random\n",
        "    \n",
        "    last_error = None\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            with httpx.Client(timeout=120) as client:\n",
        "                response = client.post(\n",
        "                    \"https://openrouter.ai/api/v1/chat/completions\",\n",
        "                    headers={\n",
        "                        \"Authorization\": f\"Bearer {api_key}\",\n",
        "                        \"Content-Type\": \"application/json\",\n",
        "                    },\n",
        "                    json={\n",
        "                        \"model\": model,\n",
        "                        \"messages\": messages,\n",
        "                    },\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "                \n",
        "                choice = data[\"choices\"][0]\n",
        "                usage = data.get(\"usage\", {})\n",
        "                \n",
        "                return {\n",
        "                    \"content\": choice[\"message\"][\"content\"],\n",
        "                    \"finish_reason\": choice.get(\"finish_reason\"),\n",
        "                    \"usage\": {\n",
        "                        \"prompt_tokens\": usage.get(\"prompt_tokens\", 0),\n",
        "                        \"completion_tokens\": usage.get(\"completion_tokens\", 0),\n",
        "                        \"total_tokens\": usage.get(\"total_tokens\", 0),\n",
        "                        \"cost\": usage.get(\"cost\", 0.0),\n",
        "                    }\n",
        "                }\n",
        "        except (httpx.HTTPStatusError, httpx.ConnectError, httpx.ReadError, httpx.RemoteProtocolError) as e:\n",
        "            last_error = e\n",
        "            # Retry on 400, 500, 502, 503, 504, 429 (OpenRouter-specific) or connection errors\n",
        "            should_retry = False\n",
        "            if isinstance(e, httpx.HTTPStatusError):\n",
        "                should_retry = e.response.status_code in (400, 429, 500, 502, 503, 504)\n",
        "            else:\n",
        "                should_retry = True  # Connection/read errors are retryable\n",
        "            \n",
        "            if should_retry and attempt < max_retries - 1:\n",
        "                delay = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
        "                print(f\"[RETRY] OpenRouter request failed (attempt {attempt + 1}/{max_retries}): {e}. Retrying in {delay:.1f}s...\")\n",
        "                time.sleep(delay)\n",
        "                continue\n",
        "            raise\n",
        "    \n",
        "    # Should not reach here, but just in case\n",
        "    raise last_error\n",
        "\n",
        "\n",
        "def run_react_agent(\n",
        "    model_name: str,\n",
        "    task_prompt: str,\n",
        "    bash_executor: BashExecutorProxy,\n",
        "    system_prompt: str,\n",
        "    max_iterations: int = 25,\n",
        "    trace_accumulator: Dict = None,\n",
        "    stop_event: \"threading.Event\" = None,\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Custom ReAct agent loop using XML tags.\n",
        "    Returns structured trace with each step containing thinking, action, observation,\n",
        "    plus token usage and finish reasons.\n",
        "    \n",
        "    If trace_accumulator is provided, steps are written to it in real-time,\n",
        "    allowing partial trace recovery on timeout.\n",
        "    \n",
        "    If stop_event is provided and set, the loop exits gracefully at the next iteration.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Task: {task_prompt}\"},\n",
        "    ]\n",
        "    \n",
        "    # Use provided accumulator or create new one\n",
        "    if trace_accumulator is not None:\n",
        "        steps = trace_accumulator.setdefault(\"steps\", [])\n",
        "        trace_accumulator[\"final\"] = None\n",
        "        trace_accumulator[\"completed\"] = False\n",
        "        trace_accumulator[\"usage\"] = {\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0, \"cost\": 0.0}\n",
        "    else:\n",
        "        steps = []\n",
        "    \n",
        "    final_step = None\n",
        "    completed = False\n",
        "    \n",
        "    # Track total usage across all iterations\n",
        "    total_usage = {\n",
        "        \"prompt_tokens\": 0,\n",
        "        \"completion_tokens\": 0,\n",
        "        \"total_tokens\": 0,\n",
        "        \"cost\": 0.0,\n",
        "    }\n",
        "    \n",
        "    for iteration in range(max_iterations):\n",
        "        # Check stop signal at start of each iteration\n",
        "        if stop_event and stop_event.is_set():\n",
        "            # Timeout was triggered - exit gracefully\n",
        "            break\n",
        "        \n",
        "        # Get model response\n",
        "        try:\n",
        "            api_response = call_openrouter(\n",
        "                model=model_name,\n",
        "                messages=messages,\n",
        "                api_key=OPENAI_API_KEY,\n",
        "            )\n",
        "            response_text = api_response[\"content\"]\n",
        "            finish_reason = api_response[\"finish_reason\"]\n",
        "            iter_usage = api_response[\"usage\"]\n",
        "            \n",
        "            # Accumulate total usage\n",
        "            total_usage[\"prompt_tokens\"] += iter_usage[\"prompt_tokens\"]\n",
        "            total_usage[\"completion_tokens\"] += iter_usage[\"completion_tokens\"]\n",
        "            total_usage[\"total_tokens\"] += iter_usage[\"total_tokens\"]\n",
        "            total_usage[\"cost\"] += iter_usage[\"cost\"]\n",
        "            \n",
        "            # Update accumulator in real-time\n",
        "            if trace_accumulator is not None:\n",
        "                trace_accumulator[\"usage\"] = total_usage.copy()\n",
        "                \n",
        "        except Exception as e:\n",
        "            steps.append({\n",
        "                \"iteration\": iteration + 1,\n",
        "                \"error\": f\"API error: {str(e)}\",\n",
        "            })\n",
        "            break\n",
        "        \n",
        "        # Parse XML response\n",
        "        thinking, action, done = parse_react_response(response_text)\n",
        "        \n",
        "        # If model includes both, execute the action and ignore premature done\n",
        "        if action:\n",
        "            # Execute bash command\n",
        "            try:\n",
        "                result = bash_executor.execute(action)\n",
        "                # Normalize result to dict for consistent storage\n",
        "                if isinstance(result, dict):\n",
        "                    observation = {\n",
        "                        \"stdout\": result.get(\"stdout\", \"\"),\n",
        "                        \"stderr\": result.get(\"stderr\", \"\"),\n",
        "                        \"exit_code\": result.get(\"exit_code\", 0),\n",
        "                    }\n",
        "                else:\n",
        "                    observation = {\n",
        "                        \"stdout\": str(result) if result else \"\",\n",
        "                        \"stderr\": \"\",\n",
        "                        \"exit_code\": 0,\n",
        "                    }\n",
        "            except Exception as e:\n",
        "                observation = {\n",
        "                    \"stdout\": \"\",\n",
        "                    \"stderr\": str(e),\n",
        "                    \"exit_code\": 1,\n",
        "                    \"error\": str(e),\n",
        "                }\n",
        "            \n",
        "            # Record this step with nested structure + usage\n",
        "            steps.append({\n",
        "                \"iteration\": iteration + 1,\n",
        "                \"thinking\": thinking,\n",
        "                \"action\": action,\n",
        "                \"observation\": observation,\n",
        "                \"raw_response\": response_text,\n",
        "                \"finish_reason\": finish_reason,\n",
        "                \"usage\": iter_usage,\n",
        "            })\n",
        "            \n",
        "            # Format observation for model (just stdout, or error info)\n",
        "            if observation.get(\"exit_code\", 0) != 0:\n",
        "                obs_text = f\"{observation['stdout']}\\n[stderr]: {observation['stderr']}\\n[exit_code]: {observation['exit_code']}\".strip()\n",
        "            else:\n",
        "                obs_text = observation[\"stdout\"].strip() if observation[\"stdout\"] else \"(empty output)\"\n",
        "            \n",
        "            # Add to conversation\n",
        "            messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"<observation>\\n{obs_text}\\n</observation>\"})\n",
        "        elif done:\n",
        "            # Task completed (only when NO action present)\n",
        "            final_step = {\n",
        "                \"iteration\": iteration + 1,\n",
        "                \"thinking\": thinking,\n",
        "                \"summary\": done,\n",
        "                \"raw_response\": response_text,\n",
        "                \"finish_reason\": finish_reason,\n",
        "                \"usage\": iter_usage,\n",
        "            }\n",
        "            completed = True\n",
        "            break\n",
        "        else:\n",
        "            # No action and no done - malformed response\n",
        "            steps.append({\n",
        "                \"iteration\": iteration + 1,\n",
        "                \"thinking\": thinking,\n",
        "                \"warning\": \"No <action> or <done> tag found\",\n",
        "                \"raw_response\": response_text,\n",
        "                \"finish_reason\": finish_reason,\n",
        "                \"usage\": iter_usage,\n",
        "            })\n",
        "            messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "            messages.append({\"role\": \"user\", \"content\": \"Please respond with either an <action> to execute or <done> if the task is complete.\"})\n",
        "    \n",
        "    result = {\n",
        "        \"steps\": steps,\n",
        "        \"final\": final_step,\n",
        "        \"iterations\": iteration + 1,\n",
        "        \"completed\": completed,\n",
        "        \"usage\": total_usage,\n",
        "    }\n",
        "    \n",
        "    # Update accumulator if provided (for timeout recovery)\n",
        "    if trace_accumulator is not None:\n",
        "        trace_accumulator[\"final\"] = final_step\n",
        "        trace_accumulator[\"iterations\"] = iteration + 1\n",
        "        trace_accumulator[\"completed\"] = completed\n",
        "        trace_accumulator[\"usage\"] = total_usage\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "async def run_single_test(\n",
        "    client: AgentDiff, \n",
        "    model_name: str, \n",
        "    test: Any, \n",
        "    system_prompt: str,\n",
        "    test_timeout_seconds: int = 300,\n",
        "    max_iterations: int = 25,\n",
        ") -> tuple:\n",
        "    \"\"\"Run a single test case using custom ReAct agent.\n",
        "    \n",
        "    Args:\n",
        "        client: AgentDiff client instance\n",
        "        model_name: Model identifier (e.g., 'openai/gpt-5-mini')\n",
        "        test: Test object with id and prompt attributes\n",
        "        system_prompt: Full system prompt including API docs\n",
        "        test_timeout_seconds: Max seconds before timeout\n",
        "        max_iterations: Max ReAct loop iterations\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (test_id, result_dict) where result_dict contains:\n",
        "            - prompt (str): Task prompt\n",
        "            - status (str): 'passed', 'failed', 'timeout', or 'error'\n",
        "            - passed (bool): Whether assertions passed\n",
        "            - score (float): Score 0-100\n",
        "            - time (float): Execution seconds\n",
        "            - failures (list[str]): Failure messages\n",
        "            - runId (str): Run UUID\n",
        "            - error (str|None): Error message if status='error'\n",
        "            - trace (dict): Execution trace containing:\n",
        "                - steps (list): Each step has iteration, thinking, action, \n",
        "                  observation, raw_response, finish_reason, usage\n",
        "                - final (dict|None): Completion step with usage\n",
        "                - iterations (int): Total iterations\n",
        "                - completed (bool): Whether agent declared done\n",
        "                - usage (dict): Total {prompt_tokens, completion_tokens, \n",
        "                  total_tokens, cost}\n",
        "            - diff (dict|None): State changes {inserts, updates, deletes}\n",
        "    \"\"\"\n",
        "    import threading\n",
        "    \n",
        "    test_id = test.id\n",
        "    prompt = test.prompt\n",
        "    response = None\n",
        "    timed_out = False\n",
        "    env = None\n",
        "    stop_event = threading.Event()  # Signal for graceful thread cancellation\n",
        "\n",
        "    try:\n",
        "        # Initialize environment\n",
        "        env = client.init_env(testId=test_id)\n",
        "        run = client.start_run(envId=env.environmentId, testId=test_id)\n",
        "\n",
        "        # Create bash executor (direct, not LangChain tool)\n",
        "        bash_executor = BashExecutorProxy(\n",
        "            env.environmentId,\n",
        "            base_url=client.base_url,\n",
        "            api_key=client.api_key,\n",
        "        )\n",
        "\n",
        "        # Execution with timeout\n",
        "        # Use trace_accumulator to capture partial trace on timeout\n",
        "        trace_accumulator = {\n",
        "            \"steps\": [], \n",
        "            \"final\": None, \n",
        "            \"completed\": False,\n",
        "            \"usage\": {\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0, \"cost\": 0.0},\n",
        "        }\n",
        "        \n",
        "        start = time.perf_counter()\n",
        "        try:\n",
        "            response = await asyncio.wait_for(\n",
        "                asyncio.to_thread(\n",
        "                    run_react_agent,\n",
        "                    model_name=model_name,\n",
        "                    task_prompt=prompt,\n",
        "                    bash_executor=bash_executor,\n",
        "                    system_prompt=system_prompt,\n",
        "                    max_iterations=max_iterations,\n",
        "                    trace_accumulator=trace_accumulator,\n",
        "                    stop_event=stop_event,\n",
        "                ),\n",
        "                timeout=test_timeout_seconds\n",
        "            )\n",
        "        except asyncio.TimeoutError:\n",
        "            timed_out = True\n",
        "            # Signal thread to stop at next iteration\n",
        "            stop_event.set()\n",
        "            # Give thread a moment to finish current operation and exit\n",
        "            await asyncio.sleep(2)\n",
        "            # Use accumulated trace (partial) instead of losing it\n",
        "            response = {\n",
        "                \"steps\": trace_accumulator.get(\"steps\", []),\n",
        "                \"final\": trace_accumulator.get(\"final\"),\n",
        "                \"iterations\": len(trace_accumulator.get(\"steps\", [])),\n",
        "                \"completed\": False,\n",
        "                \"usage\": trace_accumulator.get(\"usage\", {}),\n",
        "                \"timeout_error\": f\"Test timed out after {test_timeout_seconds} seconds\",\n",
        "            }\n",
        "        except Exception as e:\n",
        "            response = {\n",
        "                \"steps\": trace_accumulator.get(\"steps\", []),\n",
        "                \"final\": trace_accumulator.get(\"final\"),\n",
        "                \"iterations\": len(trace_accumulator.get(\"steps\", [])),\n",
        "                \"completed\": False,\n",
        "                \"usage\": trace_accumulator.get(\"usage\", {}),\n",
        "                \"error\": str(e),\n",
        "            }\n",
        "        finally:\n",
        "            execution_time = time.perf_counter() - start\n",
        "\n",
        "        # Evaluation\n",
        "        score = client.evaluate_run(runId=run.runId)\n",
        "        run_result = client.get_results_for_run(runId=run.runId)\n",
        "\n",
        "        result = {\n",
        "            \"prompt\": prompt,\n",
        "            \"status\": \"timeout\" if timed_out else run_result.status,\n",
        "            \"passed\": False if timed_out else run_result.passed,\n",
        "            \"score\": 0 if timed_out else run_result.score.get(\"percent\", 0),\n",
        "            \"time\": round(execution_time, 2),\n",
        "            \"failures\": [\"Test timed out\"] if timed_out else run_result.failures,\n",
        "            \"runId\": run.runId,\n",
        "            \"trace\": response,\n",
        "            \"diff\": getattr(run_result, \"diff\", None),\n",
        "        }\n",
        "\n",
        "        # Cleanup\n",
        "        client.delete_env(envId=env.environmentId)\n",
        "        return test_id, result\n",
        "\n",
        "    except Exception as e:\n",
        "        # Cleanup on error if environment was created\n",
        "        if env:\n",
        "            try:\n",
        "                client.delete_env(envId=env.environmentId)\n",
        "            except:\n",
        "                pass\n",
        "        return test_id, {\"passed\": False, \"score\": 0, \"status\": \"error\", \"error\": str(e)}\n",
        "\n",
        "\n",
        "async def run_benchmark_suite(\n",
        "    service: str,\n",
        "    models: list,\n",
        "    runs_per_test: int = 1,\n",
        "    max_tests: int = None,\n",
        "    max_concurrent_models: int = 1,\n",
        "    max_concurrent_tests: int = 10,\n",
        "    max_calls_per_minute: int = 90,\n",
        "    test_timeout_seconds: int = 300,\n",
        "    max_iterations: int = 25,\n",
        "    include_api_docs: bool = True,\n",
        "    checkpoint: \"BenchmarkCheckpoint\" = None,\n",
        "):\n",
        "    \"\"\"Run benchmark for a single service.\n",
        "    \n",
        "    Args:\n",
        "        service: Service to benchmark ('slack', 'box', 'calendar', 'linear')\n",
        "        models: List of model identifiers to evaluate\n",
        "        runs_per_test: Number of times to run each test\n",
        "        max_tests: Maximum number of tests to run (None = all)\n",
        "        max_concurrent_models: Max parallel model evaluations\n",
        "        max_concurrent_tests: Max parallel test executions\n",
        "        max_calls_per_minute: Rate limit for API calls\n",
        "        test_timeout_seconds: Timeout per test in seconds\n",
        "        max_iterations: Max ReAct iterations per test\n",
        "        include_api_docs: Whether to include API documentation in system prompt\n",
        "        checkpoint: Optional BenchmarkCheckpoint for resuming interrupted runs.\n",
        "                   If provided, skips already-completed tasks and saves progress incrementally.\n",
        "    \n",
        "    Returns:\n",
        "        List[dict]: List of result dicts, each containing:\n",
        "            - prompt (str): The task prompt\n",
        "            - status (str): 'passed', 'failed', 'timeout', or 'error'\n",
        "            - passed (bool): Whether the test passed\n",
        "            - score (float): Score percentage (0-100)\n",
        "            - time (float): Execution time in seconds\n",
        "            - failures (list): List of failure messages\n",
        "            - runId (str): Unique run identifier\n",
        "            - error (str|None): Error message if status='error'\n",
        "            - model (str): Model identifier used\n",
        "            - test_id (str): Test UUID (deterministic, constant across runs)\n",
        "            - test_name (str): Human-readable test name from benchmark suite\n",
        "            - service (str): Service name (e.g., 'slack', 'box')\n",
        "            - test_suite_name (str): Full test suite name (e.g., 'Slack Bench v2')\n",
        "            - include_api_docs (bool): Whether API docs were included in prompt\n",
        "            - timestamp (str): ISO format timestamp when test was run\n",
        "            - trace (dict): Execution trace containing:\n",
        "                - steps (list): List of ReAct steps, each with:\n",
        "                    - iteration (int)\n",
        "                    - thinking (str): Model's reasoning\n",
        "                    - action (str): Bash command executed\n",
        "                    - observation (dict): {stdout, stderr, exit_code}\n",
        "                    - raw_response (str): Full model response\n",
        "                    - finish_reason (str): \"stop\", \"length\" (context overflow), etc.\n",
        "                    - usage (dict): {prompt_tokens, completion_tokens, total_tokens, cost}\n",
        "                - final (dict|None): Completion step with thinking, summary, usage\n",
        "                - iterations (int): Total iterations\n",
        "                - completed (bool): Whether agent declared done\n",
        "                - usage (dict): Total tokens/cost for entire run:\n",
        "                    {prompt_tokens, completion_tokens, total_tokens, cost}\n",
        "            - diff (dict|None): State diff with inserts, updates, deletes\n",
        "    \"\"\"\n",
        "    # Get benchmark configuration for this service\n",
        "    config = get_benchmark_config(service, include_api_docs=include_api_docs)\n",
        "    test_suite_name = config[\"test_suite_name\"]\n",
        "    system_prompt = config[\"system_prompt\"]\n",
        "    run_timestamp = datetime.now().isoformat()\n",
        "    \n",
        "    client = AgentDiff(\n",
        "        #api_key=AGENT_DIFF_API_KEY,\n",
        "        #base_url=AGENT_DIFF_BASE_URL,\n",
        "    )\n",
        "    try:\n",
        "        suite_list = client.list_test_suites(name=test_suite_name)\n",
        "        if not suite_list.testSuites:\n",
        "            print(f\"[ERROR] Test suite '{test_suite_name}' not found on AgentDiff server.\")\n",
        "            return []\n",
        "\n",
        "        suite = client.get_test_suite(suite_list.testSuites[0].id, expand=True)\n",
        "        tests = suite.tests[:max_tests] if max_tests else suite.tests\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Error connecting to AgentDiff: {e}\")\n",
        "        return []\n",
        "\n",
        "    total_logical = len(tests) * len(models)\n",
        "    total_runs = total_logical * runs_per_test\n",
        "    \n",
        "    # Checkpointing: determine which tasks need to run\n",
        "    if checkpoint:\n",
        "        # Build list of all tasks, filter out completed ones\n",
        "        all_tasks_spec = []\n",
        "        for model in models:\n",
        "            for test in tests:\n",
        "                test_id = str(test.id)\n",
        "                for run_idx in range(runs_per_test):\n",
        "                    if not checkpoint.is_completed(model, test_id, run_idx):\n",
        "                        all_tasks_spec.append((model, test, run_idx))\n",
        "        \n",
        "        skipped = total_runs - len(all_tasks_spec)\n",
        "        if skipped > 0:\n",
        "            print(f\"\\n[{config['service'].upper()}] {test_suite_name} | {len(tests)} tests x {len(models)} models x {runs_per_test} runs\")\n",
        "            print(f\"[CHECKPOINT] Skipping {skipped} already completed, {len(all_tasks_spec)} remaining\")\n",
        "        else:\n",
        "            print(f\"\\n[{config['service'].upper()}] {test_suite_name} | {len(tests)} tests x {len(models)} models x {runs_per_test} runs = {total_runs} total\")\n",
        "        \n",
        "        # Use checkpoint's lock for thread safety\n",
        "        checkpoint_lock = asyncio.Lock()\n",
        "    else:\n",
        "        # No checkpoint - run all tasks\n",
        "        all_tasks_spec = [(model, test, run_idx) \n",
        "                         for model in models \n",
        "                         for test in tests \n",
        "                         for run_idx in range(runs_per_test)]\n",
        "        checkpoint_lock = None\n",
        "        print(f\"\\n[{config['service'].upper()}] {test_suite_name} | {len(tests)} tests x {len(models)} models x {runs_per_test} runs = {total_runs} total\")\n",
        "\n",
        "    semaphore = asyncio.Semaphore(max_concurrent_models * max_concurrent_tests)\n",
        "\n",
        "    # rate limiting state (per minute window)\n",
        "    window_seconds = 60\n",
        "    window_start = time.monotonic()\n",
        "    calls_in_window = 0\n",
        "    rate_lock = asyncio.Lock()\n",
        "\n",
        "    async def acquire_rate_slot():\n",
        "        nonlocal window_start, calls_in_window\n",
        "        while True:\n",
        "            async with rate_lock:\n",
        "                now = time.monotonic()\n",
        "                # reset window if needed\n",
        "                if now - window_start >= window_seconds:\n",
        "                    window_start = now\n",
        "                    calls_in_window = 0\n",
        "\n",
        "                if calls_in_window < max_calls_per_minute:\n",
        "                    calls_in_window += 1\n",
        "                    return  # allowed to proceed\n",
        "\n",
        "                # need to wait until current window ends\n",
        "                sleep_for = window_seconds - (now - window_start)\n",
        "            # sleep outside the lock\n",
        "            if sleep_for > 0:\n",
        "                await asyncio.sleep(sleep_for)\n",
        "\n",
        "    # Progress tracking state\n",
        "    completed_results = []\n",
        "    results_lock = asyncio.Lock()\n",
        "    \n",
        "    # Create progress bar with model names\n",
        "    model_names = [m.split(\"/\")[-1][:12] for m in models]\n",
        "    initial_desc = f\"{config['service'].upper()} | \" + \" | \".join(f\"{m}: 0/0\" for m in model_names)\n",
        "    \n",
        "    # Progress bar shows remaining tasks (may be less than total if resuming)\n",
        "    tasks_to_run = len(all_tasks_spec)\n",
        "    pbar = tqdm(\n",
        "        total=tasks_to_run,\n",
        "        desc=initial_desc,\n",
        "        unit=\"test\",\n",
        "        leave=True,\n",
        "        dynamic_ncols=True,\n",
        "        mininterval=0.05,  # More frequent updates\n",
        "    )\n",
        "    pbar.refresh()  # Force initial display\n",
        "    \n",
        "    async def update_progress():\n",
        "        \"\"\"Update progress bar with current stats per model.\"\"\"\n",
        "        async with results_lock:\n",
        "            n = len(completed_results)\n",
        "            if n > 0:\n",
        "                # Build per-model stats\n",
        "                model_stats = {}\n",
        "                for r in completed_results:\n",
        "                    m = r.get(\"model\", \"unknown\").split(\"/\")[-1][:12]  # Short model name\n",
        "                    if m not in model_stats:\n",
        "                        model_stats[m] = {\"passed\": 0, \"total\": 0}\n",
        "                    model_stats[m][\"total\"] += 1\n",
        "                    if r.get(\"passed\"):\n",
        "                        model_stats[m][\"passed\"] += 1\n",
        "                \n",
        "                # Format: \"model1: 5/10 | model2: 3/8\"\n",
        "                model_parts = [f\"{m}: {s['passed']}/{s['total']}\" for m, s in model_stats.items()]\n",
        "                model_str = \" | \".join(model_parts)\n",
        "                \n",
        "                pbar.set_description(f\"{config['service'].upper()} | {model_str}\")\n",
        "                pbar.refresh()\n",
        "\n",
        "    async def worker(model_name, test, run_idx):\n",
        "        await acquire_rate_slot()\n",
        "        async with semaphore:\n",
        "            tid, res = await run_single_test(\n",
        "                client, model_name, test, system_prompt,\n",
        "                test_timeout_seconds=test_timeout_seconds,\n",
        "                max_iterations=max_iterations,\n",
        "            )\n",
        "            res[\"model\"] = model_name\n",
        "            res[\"test_id\"] = str(tid)\n",
        "            res[\"test_name\"] = test.name\n",
        "            res[\"run_index\"] = run_idx  # Track which run this is\n",
        "            \n",
        "            # Add metadata immediately (needed for checkpoint)\n",
        "            res[\"service\"] = config[\"service\"]\n",
        "            res[\"test_suite_name\"] = test_suite_name\n",
        "            res[\"include_api_docs\"] = include_api_docs\n",
        "            res[\"timestamp\"] = run_timestamp\n",
        "            \n",
        "            # Track result and update progress\n",
        "            async with results_lock:\n",
        "                completed_results.append(res)\n",
        "            \n",
        "            # Save to checkpoint if enabled\n",
        "            if checkpoint and checkpoint_lock:\n",
        "                async with checkpoint_lock:\n",
        "                    checkpoint.mark_completed(model_name, str(tid), run_idx, res.copy())\n",
        "                    checkpoint.save()  # Incremental save after each test\n",
        "            \n",
        "            pbar.update(1)\n",
        "            await update_progress()\n",
        "            pbar.refresh()  # Force display refresh in Jupyter\n",
        "            \n",
        "            # Log failures to tqdm (won't mess up progress bar)\n",
        "            if not res.get(\"passed\"):\n",
        "                name_short = test.name[:35] + \"...\" if len(test.name) > 35 else test.name\n",
        "                model_short = model_name.split(\"/\")[-1][:15]  # e.g., \"anthropic/claude-3\" -> \"claude-3\"\n",
        "                if res.get(\"status\") == \"timeout\":\n",
        "                    tqdm.write(f\"[TIMEOUT] {model_short} | {name_short} | {res.get('time', 0):.1f}s\")\n",
        "                elif res.get(\"status\") == \"error\":\n",
        "                    tqdm.write(f\"[ERROR] {model_short} | {name_short} | {res.get('error', 'unknown')[:50]}\")\n",
        "                else:\n",
        "                    tqdm.write(f\"[FAIL] {model_short} | {name_short} | {res.get('score')}%\")\n",
        "            \n",
        "            return res\n",
        "\n",
        "    # Create tasks from the (possibly filtered) task spec\n",
        "    tasks = [worker(model, test, run_idx) for model, test, run_idx in all_tasks_spec]\n",
        "\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    pbar.close()\n",
        "    \n",
        "    # Note: Metadata is already added in the worker function for checkpoint support\n",
        "    \n",
        "    # Combine with checkpoint results if resuming\n",
        "    if checkpoint:\n",
        "        # Get all results from checkpoint (includes both old and newly added)\n",
        "        all_results = checkpoint.get_results()\n",
        "        # Filter to only this service's results\n",
        "        service_results = [r for r in all_results if r.get(\"service\") == config[\"service\"]]\n",
        "        \n",
        "        # Final summary (includes resumed results)\n",
        "        passed = sum(1 for r in service_results if r.get(\"passed\"))\n",
        "        avg_score = sum(r.get(\"score\", 0) for r in service_results) / len(service_results) if service_results else 0\n",
        "        print(f\"{config['service'].upper()} Complete: {passed}/{len(service_results)} passed ({avg_score:.1f}% avg)\")\n",
        "        if len(results) < len(service_results):\n",
        "            print(f\"  (includes {len(service_results) - len(results)} resumed from checkpoint)\")\n",
        "        \n",
        "        return service_results\n",
        "    else:\n",
        "        # Final summary\n",
        "        passed = sum(1 for r in results if r.get(\"passed\"))\n",
        "        avg_score = sum(r.get(\"score\", 0) for r in results) / len(results) if results else 0\n",
        "        print(f\"{config['service'].upper()} Complete: {passed}/{len(results)} passed ({avg_score:.1f}% avg)\")\n",
        "        \n",
        "        return results\n",
        "\n",
        "\n",
        "async def run_all_benchmarks(\n",
        "    models: list,\n",
        "    services: list = None,\n",
        "    runs_per_test: int = 1,\n",
        "    max_tests: int = None,\n",
        "    max_concurrent_models: int = 1,\n",
        "    max_concurrent_tests: int = 10,\n",
        "    max_calls_per_minute: int = 90,\n",
        "    test_timeout_seconds: int = 300,\n",
        "    max_iterations: int = 25,\n",
        "    include_api_docs: bool = True,\n",
        "    checkpoint: \"BenchmarkCheckpoint\" = None,\n",
        "):\n",
        "    \"\"\"Run benchmarks for multiple services.\n",
        "    \n",
        "    Args:\n",
        "        models: List of model identifiers to evaluate\n",
        "        services: List of services to benchmark. If None, runs all available services.\n",
        "                  Options: ['slack', 'box', 'calendar', 'linear']\n",
        "        runs_per_test: Number of times to run each test\n",
        "        max_tests: Maximum number of tests to run per service (None = all)\n",
        "        max_concurrent_models: Max parallel model evaluations\n",
        "        max_concurrent_tests: Max parallel test executions\n",
        "        max_calls_per_minute: Rate limit for API calls\n",
        "        test_timeout_seconds: Timeout per test in seconds\n",
        "        max_iterations: Max ReAct iterations per test\n",
        "        include_api_docs: Whether to include API documentation in system prompt\n",
        "        checkpoint: Optional BenchmarkCheckpoint for resuming interrupted runs\n",
        "    \n",
        "    Returns:\n",
        "        Dict[str, List[dict]]: Mapping of service name to list of results.\n",
        "            Each result dict contains (see run_benchmark_suite for full schema):\n",
        "            - prompt, status, passed, score, time, failures, error\n",
        "            - runId, model, test_id, test_name, service, test_suite_name\n",
        "            - include_api_docs (bool), timestamp (ISO format)\n",
        "            - trace: {steps, final, iterations, completed, usage}\n",
        "              - Each step includes: finish_reason, usage (per-iteration tokens/cost)\n",
        "              - usage: Total {prompt_tokens, completion_tokens, total_tokens, cost}\n",
        "            - diff: {inserts, updates, deletes}\n",
        "    \"\"\"\n",
        "    if services is None:\n",
        "        services = list(BENCHMARK_CONFIGS.keys())\n",
        "    \n",
        "    docs_status = \"with API docs\" if include_api_docs else \"NO API docs\"\n",
        "    print(f\"Benchmarks: {', '.join(s.upper() for s in services)} | {len(models)} models | {docs_status} | {test_timeout_seconds}s timeout\")\n",
        "    \n",
        "    all_results = {}\n",
        "    for service in services:\n",
        "        try:\n",
        "            results = await run_benchmark_suite(\n",
        "                service=service,\n",
        "                models=models,\n",
        "                runs_per_test=runs_per_test,\n",
        "                max_tests=max_tests,\n",
        "                max_concurrent_models=max_concurrent_models,\n",
        "                max_concurrent_tests=max_concurrent_tests,\n",
        "                max_calls_per_minute=max_calls_per_minute,\n",
        "                test_timeout_seconds=test_timeout_seconds,\n",
        "                max_iterations=max_iterations,\n",
        "                include_api_docs=include_api_docs,\n",
        "                checkpoint=checkpoint,\n",
        "            )\n",
        "            all_results[service] = results\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Error running {service} benchmark: {e}\")\n",
        "            all_results[service] = []\n",
        "    \n",
        "    # Overall summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"OVERALL SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    total_passed = 0\n",
        "    total_tests = 0\n",
        "    for service, results in all_results.items():\n",
        "        if results:\n",
        "            passed = sum(1 for r in results if r.get(\"passed\"))\n",
        "            total = len(results)\n",
        "            total_passed += passed\n",
        "            total_tests += total\n",
        "            print(f\"  {service.upper()}: {passed}/{total} passed\")\n",
        "    \n",
        "    if total_tests > 0:\n",
        "        print(f\"\\n  TOTAL: {total_passed}/{total_tests} passed ({100*total_passed/total_tests:.1f}%)\")\n",
        "    \n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ Checkpointing System ============\n",
        "# Tracks progress and allows resuming interrupted benchmark runs\n",
        "\n",
        "import json\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Optional, Set, Tuple\n",
        "\n",
        "CHECKPOINT_DIR = Path(\"evaluation_outputs/checkpoints\")\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "class BenchmarkCheckpoint:\n",
        "    \"\"\"Manages checkpoint state for benchmark runs.\n",
        "    \n",
        "    Tracks which (model, test_id, run_index) combinations have been completed,\n",
        "    allowing runs to be resumed after interruption.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, checkpoint_path: Optional[Path] = None, run_name: str = None):\n",
        "        \"\"\"Initialize checkpoint manager.\n",
        "        \n",
        "        Args:\n",
        "            checkpoint_path: Path to checkpoint file. If None, generates based on run_name.\n",
        "            run_name: Name for this run (used to generate checkpoint filename if path not given).\n",
        "        \"\"\"\n",
        "        if checkpoint_path:\n",
        "            self.checkpoint_path = Path(checkpoint_path)\n",
        "        else:\n",
        "            name = run_name or datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            self.checkpoint_path = CHECKPOINT_DIR / f\"checkpoint_{name}.json\"\n",
        "        \n",
        "        self.completed: Set[str] = set()  # Set of \"model|test_id|run_idx\" keys\n",
        "        self.results: list = []  # Accumulated results\n",
        "        self.metadata: dict = {}  # Run metadata\n",
        "        self._lock = None  # Will be set to asyncio.Lock() when running async\n",
        "        \n",
        "    def _make_key(self, model: str, test_id: str, run_idx: int) -> str:\n",
        "        \"\"\"Create unique key for a (model, test, run) combination.\"\"\"\n",
        "        return f\"{model}|{test_id}|{run_idx}\"\n",
        "    \n",
        "    def is_completed(self, model: str, test_id: str, run_idx: int) -> bool:\n",
        "        \"\"\"Check if a specific (model, test, run) has been completed.\"\"\"\n",
        "        return self._make_key(model, test_id, run_idx) in self.completed\n",
        "    \n",
        "    def mark_completed(self, model: str, test_id: str, run_idx: int, result: dict):\n",
        "        \"\"\"Mark a (model, test, run) as completed and store result.\"\"\"\n",
        "        key = self._make_key(model, test_id, run_idx)\n",
        "        self.completed.add(key)\n",
        "        result[\"_checkpoint_key\"] = key  # Store key in result for deduplication\n",
        "        self.results.append(result)\n",
        "    \n",
        "    def save(self):\n",
        "        \"\"\"Save checkpoint to disk.\"\"\"\n",
        "        data = {\n",
        "            \"completed\": list(self.completed),\n",
        "            \"results\": self.results,\n",
        "            \"metadata\": self.metadata,\n",
        "            \"saved_at\": datetime.now().isoformat(),\n",
        "        }\n",
        "        \n",
        "        def safe_serialize(obj):\n",
        "            if isinstance(obj, bytes):\n",
        "                return obj.decode('utf-8', errors='replace')\n",
        "            return str(obj)\n",
        "        \n",
        "        with open(self.checkpoint_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, default=safe_serialize, ensure_ascii=False)\n",
        "    \n",
        "    def load(self) -> bool:\n",
        "        \"\"\"Load checkpoint from disk. Returns True if loaded successfully.\"\"\"\n",
        "        if not self.checkpoint_path.exists():\n",
        "            return False\n",
        "        \n",
        "        try:\n",
        "            with open(self.checkpoint_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            \n",
        "            self.completed = set(data.get(\"completed\", []))\n",
        "            self.results = data.get(\"results\", [])\n",
        "            self.metadata = data.get(\"metadata\", {})\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"[CHECKPOINT] Warning: Failed to load checkpoint: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def get_remaining_tasks(\n",
        "        self, \n",
        "        models: list, \n",
        "        tests: list, \n",
        "        runs_per_test: int\n",
        "    ) -> list:\n",
        "        \"\"\"Get list of (model, test, run_idx) tuples that haven't been completed.\"\"\"\n",
        "        remaining = []\n",
        "        for model in models:\n",
        "            for test in tests:\n",
        "                test_id = str(test.id)\n",
        "                for run_idx in range(runs_per_test):\n",
        "                    if not self.is_completed(model, test_id, run_idx):\n",
        "                        remaining.append((model, test, run_idx))\n",
        "        return remaining\n",
        "    \n",
        "    def get_completed_count(self) -> int:\n",
        "        \"\"\"Get number of completed tasks.\"\"\"\n",
        "        return len(self.completed)\n",
        "    \n",
        "    def get_results(self) -> list:\n",
        "        \"\"\"Get all accumulated results (removing internal checkpoint keys).\"\"\"\n",
        "        results = []\n",
        "        for r in self.results:\n",
        "            r_clean = {k: v for k, v in r.items() if not k.startswith(\"_checkpoint\")}\n",
        "            results.append(r_clean)\n",
        "        return results\n",
        "    \n",
        "    def summary(self) -> str:\n",
        "        \"\"\"Get summary string of checkpoint state.\"\"\"\n",
        "        return f\"Checkpoint: {self.get_completed_count()} completed, {len(self.results)} results\"\n",
        "\n",
        "\n",
        "def get_or_create_checkpoint(\n",
        "    checkpoint_path: Optional[str] = None,\n",
        "    resume: bool = True\n",
        ") -> BenchmarkCheckpoint:\n",
        "    \"\"\"Get or create a checkpoint for the current run.\n",
        "    \n",
        "    Args:\n",
        "        checkpoint_path: Path to checkpoint file. If None, creates new timestamped checkpoint.\n",
        "        resume: If True and checkpoint exists, load it. If False, start fresh.\n",
        "    \n",
        "    Returns:\n",
        "        BenchmarkCheckpoint instance\n",
        "    \"\"\"\n",
        "    checkpoint = BenchmarkCheckpoint(\n",
        "        checkpoint_path=Path(checkpoint_path) if checkpoint_path else None\n",
        "    )\n",
        "    \n",
        "    if resume and checkpoint.load():\n",
        "        print(f\"[CHECKPOINT] Resumed from {checkpoint.checkpoint_path}\")\n",
        "        print(f\"[CHECKPOINT] {checkpoint.summary()}\")\n",
        "    else:\n",
        "        print(f\"[CHECKPOINT] Starting fresh run, saving to {checkpoint.checkpoint_path}\")\n",
        "    \n",
        "    return checkpoint\n",
        "\n",
        "\n",
        "def list_checkpoints(checkpoint_dir: Path = CHECKPOINT_DIR) -> list:\n",
        "    \"\"\"List all available checkpoints.\"\"\"\n",
        "    checkpoints = sorted(checkpoint_dir.glob(\"checkpoint_*.json\"), reverse=True)\n",
        "    print(f\"Found {len(checkpoints)} checkpoints in {checkpoint_dir}:\")\n",
        "    for i, cp in enumerate(checkpoints[:10]):  # Show last 10\n",
        "        size_kb = cp.stat().st_size / 1024\n",
        "        # Load to get summary\n",
        "        try:\n",
        "            with open(cp) as f:\n",
        "                data = json.load(f)\n",
        "            n_completed = len(data.get(\"completed\", []))\n",
        "            saved_at = data.get(\"saved_at\", \"unknown\")\n",
        "            print(f\"  [{i}] {cp.name} | {n_completed} completed | {saved_at} | {size_kb:.1f}KB\")\n",
        "        except:\n",
        "            print(f\"  [{i}] {cp.name} | {size_kb:.1f}KB (error reading)\")\n",
        "    return checkpoints\n",
        "\n",
        "print(\"[OK] Checkpoint system loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Models to evaluate (uncomment to include)\n",
        "\n",
        "MODELS = [\n",
        "    #\"openai/gpt-5-mini\",\n",
        "    # \"anthropic/claude-haiku-4.5\",\n",
        "    # \"anthropic/claude-sonnet-4.5\",\n",
        "    # \"anthropic/claude-opus-4.5\",\n",
        "    # \"x-ai/grok-4.1-fast\",\n",
        "    #\"deepseek/deepseek-v3.2\",\n",
        "    # \"moonshotai/kimi-k2-0905\",\n",
        "    # \"qwen/qwen3-vl-235b-a22b-instruct\",\n",
        "     \"google/gemini-3-flash-preview\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "812e87c0",
        "outputId": "914dd2f4-aabf-4478-8fa5-8a1b9728ab58"
      },
      "outputs": [],
      "source": [
        "# Runtime Settings (all passed to benchmark functions)\n",
        "\n",
        "BENCHMARK_SETTINGS = {\n",
        "    \"models\": MODELS,                    # Models to evaluate (from cell above)\n",
        "    \"runs_per_test\": 1,                  # Number of runs per test\n",
        "    \"max_tests\": None,                   # None = all tests, or set a limit\n",
        "    \"max_concurrent_models\": 1,          # Parallel model evaluations\n",
        "    \"max_concurrent_tests\": 10,          # Parallel test executions\n",
        "    \"max_calls_per_minute\": 480,         # API rate limit\n",
        "    \"test_timeout_seconds\": 480,         # 8 minutes per test\n",
        "    \"max_iterations\": 40,                # Max ReAct iterations\n",
        "    \"include_api_docs\": True,           # Include API docs in prompt (False = agent explores)\n",
        "}\n",
        "\n",
        "# ============ Checkpointing (optional) ============\n",
        "# Enable to resume interrupted runs. Checkpoint saves after each test.\n",
        "\n",
        "# Option 1: Fresh run with new checkpoint\n",
        "checkpoint = get_or_create_checkpoint(resume=False)\n",
        "\n",
        "# Option 2: Resume from latest checkpoint (comment out Option 1)\n",
        "# checkpoints = list_checkpoints()  # List available checkpoints\n",
        "# checkpoint = get_or_create_checkpoint(checkpoint_path=str(checkpoints[0]), resume=True)\n",
        "\n",
        "# Option 3: No checkpointing (comment out both options above)\n",
        "# checkpoint = None\n",
        "\n",
        "# Add checkpoint to settings\n",
        "BENCHMARK_SETTINGS[\"checkpoint\"] = checkpoint\n",
        "\n",
        "# ============ Run Benchmark ============\n",
        "\n",
        "# Single service:\n",
        "#results = await run_benchmark_suite(service=\"calendar\", **BENCHMARK_SETTINGS)\n",
        "\n",
        "# Multiple services:\n",
        "all_results = await run_all_benchmarks(services=[\"linear\", \"calendar\"], **BENCHMARK_SETTINGS)\n",
        "\n",
        "# All services:\n",
        "#all_results = await run_all_benchmarks(**BENCHMARK_SETTINGS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ Analysis Functions ============\n",
        "# Reusable functions for analyzing benchmark results\n",
        "\n",
        "def styled_display(df, format_str=\"{:.1f}%\", gradient_cols=None):\n",
        "    \"\"\"Display dataframe with styling if jinja2 is available, otherwise plain.\"\"\"\n",
        "    try:\n",
        "        styled = df.style.format(format_str)\n",
        "        if gradient_cols:\n",
        "            cols = [c for c in gradient_cols if c in df.columns]\n",
        "            if cols:\n",
        "                styled = styled.background_gradient(cmap=\"RdYlGn\", vmin=0, vmax=100, subset=cols)\n",
        "        display(styled)\n",
        "    except (ImportError, AttributeError):\n",
        "        display(df)\n",
        "\n",
        "\n",
        "def analyze_results(results: list):\n",
        "    \"\"\"Display comprehensive analysis tables for benchmark results.\n",
        "    \n",
        "    Args:\n",
        "        results: List of result dicts from benchmark runs\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        print(\"No results to analyze.\")\n",
        "        return\n",
        "    \n",
        "    df = pd.DataFrame(results)\n",
        "    \n",
        "    # Helper to format model names (shorter)\n",
        "    df[\"model_short\"] = df[\"model\"].apply(lambda x: x.split(\"/\")[-1] if \"/\" in str(x) else x)\n",
        "\n",
        "    # ============ 1. Overall Leaderboard by Model ============\n",
        "    display(HTML(\"<h3>1. Overall Results by Model</h3>\"))\n",
        "    leaderboard = df.groupby(\"model_short\").agg(\n",
        "        passed=(\"passed\", \"sum\"),\n",
        "        total=(\"passed\", \"count\"),\n",
        "        avg_score=(\"score\", \"mean\"),\n",
        "        avg_time=(\"time\", \"mean\")\n",
        "    ).reset_index()\n",
        "    leaderboard[\"pass_rate\"] = (leaderboard[\"passed\"] / leaderboard[\"total\"] * 100).round(1)\n",
        "    leaderboard = leaderboard.sort_values(\"pass_rate\", ascending=False)\n",
        "    display(leaderboard)\n",
        "\n",
        "    # ============ 2. Results by Model and Service ============\n",
        "    display(HTML(\"<h3>2. Results by Model and Service</h3>\"))\n",
        "    by_model_service = df.groupby([\"model_short\", \"service\"]).agg(\n",
        "        passed=(\"passed\", \"sum\"),\n",
        "        total=(\"passed\", \"count\"),\n",
        "        avg_score=(\"score\", \"mean\"),\n",
        "    ).reset_index()\n",
        "    by_model_service[\"pass_rate\"] = (by_model_service[\"passed\"] / by_model_service[\"total\"] * 100).round(1)\n",
        "    \n",
        "    # Pivot for better readability\n",
        "    pivot_pass_rate = by_model_service.pivot(index=\"model_short\", columns=\"service\", values=\"pass_rate\")\n",
        "    pivot_pass_rate[\"OVERALL\"] = leaderboard.set_index(\"model_short\")[\"pass_rate\"]\n",
        "    pivot_pass_rate = pivot_pass_rate.sort_values(\"OVERALL\", ascending=False)\n",
        "    styled_display(pivot_pass_rate, gradient_cols=list(pivot_pass_rate.columns))\n",
        "\n",
        "    # ============ 3. Results by Service (all models) ============\n",
        "    display(HTML(\"<h3>3. Results by Service (All Models)</h3>\"))\n",
        "    by_service = df.groupby(\"service\").agg(\n",
        "        passed=(\"passed\", \"sum\"),\n",
        "        total=(\"passed\", \"count\"),\n",
        "        avg_score=(\"score\", \"mean\"),\n",
        "        avg_time=(\"time\", \"mean\")\n",
        "    ).reset_index()\n",
        "    by_service[\"pass_rate\"] = (by_service[\"passed\"] / by_service[\"total\"] * 100).round(1)\n",
        "    by_service = by_service.sort_values(\"pass_rate\", ascending=False)\n",
        "    display(by_service)\n",
        "\n",
        "    # ============ 4. With Docs vs Without Docs ============\n",
        "    if \"include_api_docs\" in df.columns and df[\"include_api_docs\"].nunique() > 1:\n",
        "        display(HTML(\"<h3>4. With API Docs vs Without API Docs</h3>\"))\n",
        "        \n",
        "        # Overall by docs\n",
        "        by_docs = df.groupby(\"include_api_docs\").agg(\n",
        "            passed=(\"passed\", \"sum\"),\n",
        "            total=(\"passed\", \"count\"),\n",
        "            avg_score=(\"score\", \"mean\"),\n",
        "            avg_time=(\"time\", \"mean\")\n",
        "        ).reset_index()\n",
        "        by_docs[\"pass_rate\"] = (by_docs[\"passed\"] / by_docs[\"total\"] * 100).round(1)\n",
        "        by_docs[\"include_api_docs\"] = by_docs[\"include_api_docs\"].map({True: \"With Docs\", False: \"Without Docs\"})\n",
        "        display(by_docs)\n",
        "        \n",
        "        # By model and docs\n",
        "        display(HTML(\"<h4>4a. By Model: With vs Without Docs</h4>\"))\n",
        "        by_model_docs = df.groupby([\"model_short\", \"include_api_docs\"]).agg(\n",
        "            passed=(\"passed\", \"sum\"),\n",
        "            total=(\"passed\", \"count\"),\n",
        "            avg_score=(\"score\", \"mean\"),\n",
        "        ).reset_index()\n",
        "        by_model_docs[\"pass_rate\"] = (by_model_docs[\"passed\"] / by_model_docs[\"total\"] * 100).round(1)\n",
        "        by_model_docs[\"docs\"] = by_model_docs[\"include_api_docs\"].map({True: \"With Docs\", False: \"Without Docs\"})\n",
        "        \n",
        "        pivot_docs = by_model_docs.pivot(index=\"model_short\", columns=\"docs\", values=\"pass_rate\")\n",
        "        if \"With Docs\" in pivot_docs.columns and \"Without Docs\" in pivot_docs.columns:\n",
        "            pivot_docs[\"Delta\"] = pivot_docs[\"With Docs\"] - pivot_docs[\"Without Docs\"]\n",
        "            pivot_docs = pivot_docs.sort_values(\"Delta\", ascending=False)\n",
        "        styled_display(pivot_docs, gradient_cols=[\"With Docs\", \"Without Docs\"])\n",
        "        \n",
        "        # By service and docs\n",
        "        display(HTML(\"<h4>4b. By Service: With vs Without Docs</h4>\"))\n",
        "        by_service_docs = df.groupby([\"service\", \"include_api_docs\"]).agg(\n",
        "            passed=(\"passed\", \"sum\"),\n",
        "            total=(\"passed\", \"count\"),\n",
        "            avg_score=(\"score\", \"mean\"),\n",
        "        ).reset_index()\n",
        "        by_service_docs[\"pass_rate\"] = (by_service_docs[\"passed\"] / by_service_docs[\"total\"] * 100).round(1)\n",
        "        by_service_docs[\"docs\"] = by_service_docs[\"include_api_docs\"].map({True: \"With Docs\", False: \"Without Docs\"})\n",
        "        \n",
        "        pivot_service_docs = by_service_docs.pivot(index=\"service\", columns=\"docs\", values=\"pass_rate\")\n",
        "        if \"With Docs\" in pivot_service_docs.columns and \"Without Docs\" in pivot_service_docs.columns:\n",
        "            pivot_service_docs[\"Delta\"] = pivot_service_docs[\"With Docs\"] - pivot_service_docs[\"Without Docs\"]\n",
        "        styled_display(pivot_service_docs, gradient_cols=[\"With Docs\", \"Without Docs\"])\n",
        "    else:\n",
        "        docs_status = df[\"include_api_docs\"].iloc[0] if \"include_api_docs\" in df.columns else \"unknown\"\n",
        "        print(f\"\\n[Note] All results have include_api_docs={docs_status}, no comparison available.\")\n",
        "\n",
        "    # ============ 5. Usage Summary ============\n",
        "    display(HTML(\"<h3>5. Usage Summary</h3>\"))\n",
        "    \n",
        "    # Extract usage data into dataframe columns\n",
        "    def extract_tokens(row):\n",
        "        trace = row[\"trace\"] if \"trace\" in row and isinstance(row[\"trace\"], dict) else {}\n",
        "        usage = trace.get(\"usage\", {}) if isinstance(trace, dict) else {}\n",
        "        return usage.get(\"total_tokens\", 0) if isinstance(usage, dict) else 0\n",
        "    \n",
        "    def extract_cost(row):\n",
        "        trace = row[\"trace\"] if \"trace\" in row and isinstance(row[\"trace\"], dict) else {}\n",
        "        usage = trace.get(\"usage\", {}) if isinstance(trace, dict) else {}\n",
        "        return usage.get(\"cost\", 0) if isinstance(usage, dict) else 0\n",
        "    \n",
        "    df[\"total_tokens\"] = df.apply(extract_tokens, axis=1)\n",
        "    df[\"cost\"] = df.apply(extract_cost, axis=1)\n",
        "    \n",
        "    print(f\"Total: {df['total_tokens'].sum():,.0f} tokens | ${df['cost'].sum():.4f} USD\")\n",
        "    \n",
        "    # Usage by model\n",
        "    display(HTML(\"<h4>5a. Usage by Model</h4>\"))\n",
        "    usage_by_model = df.groupby(\"model_short\").agg(\n",
        "        tests=(\"model_short\", \"count\"),\n",
        "        tokens=(\"total_tokens\", \"sum\"),\n",
        "        cost=(\"cost\", \"sum\"),\n",
        "    ).reset_index()\n",
        "    usage_by_model[\"tokens_per_test\"] = (usage_by_model[\"tokens\"] / usage_by_model[\"tests\"]).round(0).astype(int)\n",
        "    usage_by_model[\"cost_per_test\"] = (usage_by_model[\"cost\"] / usage_by_model[\"tests\"]).round(4)\n",
        "    usage_by_model = usage_by_model.sort_values(\"cost\", ascending=False)\n",
        "    display(usage_by_model)\n",
        "    \n",
        "    # Usage by model and include_api_docs\n",
        "    if \"include_api_docs\" in df.columns and df[\"include_api_docs\"].nunique() > 1:\n",
        "        display(HTML(\"<h4>5b. Usage by Model: With vs Without Docs</h4>\"))\n",
        "        usage_by_model_docs = df.groupby([\"model_short\", \"include_api_docs\"]).agg(\n",
        "            tests=(\"model_short\", \"count\"),\n",
        "            tokens=(\"total_tokens\", \"sum\"),\n",
        "            cost=(\"cost\", \"sum\"),\n",
        "        ).reset_index()\n",
        "        usage_by_model_docs[\"tokens_per_test\"] = (usage_by_model_docs[\"tokens\"] / usage_by_model_docs[\"tests\"]).round(0).astype(int)\n",
        "        usage_by_model_docs[\"cost_per_test\"] = (usage_by_model_docs[\"cost\"] / usage_by_model_docs[\"tests\"]).round(4)\n",
        "        usage_by_model_docs[\"docs\"] = usage_by_model_docs[\"include_api_docs\"].map({True: \"With Docs\", False: \"Without Docs\"})\n",
        "        \n",
        "        # Pivot for comparison\n",
        "        pivot_tokens = usage_by_model_docs.pivot(index=\"model_short\", columns=\"docs\", values=\"tokens_per_test\")\n",
        "        pivot_cost = usage_by_model_docs.pivot(index=\"model_short\", columns=\"docs\", values=\"cost_per_test\")\n",
        "        \n",
        "        print(\"Tokens per test:\")\n",
        "        display(pivot_tokens)\n",
        "        print(\"\\nCost per test:\")\n",
        "        display(pivot_cost)\n",
        "\n",
        "\n",
        "def load_and_analyze(file_path: str = None):\n",
        "    \"\"\"Load results from a JSON file and analyze them.\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to JSON file. If None, lists available files.\n",
        "    \n",
        "    Returns:\n",
        "        list: Loaded results (or None if just listing files)\n",
        "    \"\"\"\n",
        "    if file_path is None:\n",
        "        files = list_result_files()\n",
        "        print(\"\\nUsage: load_and_analyze('path/to/file.json')\")\n",
        "        print(\"   or: load_and_analyze(str(files[0]))\")\n",
        "        return None\n",
        "    \n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    print(f\"Loaded {len(results)} results from {file_path}\\n\")\n",
        "    analyze_results(results)\n",
        "    return results\n",
        "\n",
        "print(\"[OK] Analysis functions loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle both single service (results = list) and multi-service (all_results = dict)\n",
        "if 'all_results' in dir() and all_results:\n",
        "    # Flatten all_results dict into a single list\n",
        "    results_to_save = []\n",
        "    for service, service_results in all_results.items():\n",
        "        results_to_save.extend(service_results)\n",
        "elif 'results' in dir() and results:\n",
        "    results_to_save = results\n",
        "else:\n",
        "    results_to_save = []\n",
        "\n",
        "if results_to_save:\n",
        "    # Analyze results (uses analyze_results function from cell below)\n",
        "    analyze_results(results_to_save)\n",
        "    \n",
        "    # Save results to JSON\n",
        "    ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    output_path = OUTPUT_DIR / f\"full_results_{ts}.json\"\n",
        "\n",
        "    def safe_serialize(obj):\n",
        "        if isinstance(obj, bytes):\n",
        "            return obj.decode('utf-8', errors='replace')\n",
        "        return str(obj)\n",
        "\n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results_to_save, f, indent=2, default=safe_serialize, ensure_ascii=False)\n",
        "        print(f\"\\nResults saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving JSON: {e}\")\n",
        "else:\n",
        "    print(\"No results generated.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility: Merge Multiple Result Files\n",
        "\n",
        "def list_result_files(output_dir: Path = OUTPUT_DIR) -> list:\n",
        "    \"\"\"List all result JSON files in the output directory.\"\"\"\n",
        "    files = sorted(output_dir.glob(\"full_results_*.json\"))\n",
        "    print(f\"Found {len(files)} result files in {output_dir}:\")\n",
        "    for i, f in enumerate(files):\n",
        "        size_kb = f.stat().st_size / 1024\n",
        "        print(f\"  [{i}] {f.name} ({size_kb:.1f} KB)\")\n",
        "    return files\n",
        "\n",
        "\n",
        "def merge_result_files(\n",
        "    files: list = None,\n",
        "    output_dir: Path = OUTPUT_DIR,\n",
        "    output_name: str = None,\n",
        "    deduplicate: bool = False,\n",
        ") -> list:\n",
        "    \"\"\"Merge multiple result JSON files into one.\n",
        "    \n",
        "    Args:\n",
        "        files: List of file paths to merge. If None, merges all files in output_dir.\n",
        "        output_dir: Directory containing result files (used if files=None)\n",
        "        output_name: Output filename. If None, generates timestamped name.\n",
        "        deduplicate: If True, removes duplicate entries (same test_id + model + timestamp)\n",
        "    \n",
        "    Returns:\n",
        "        list: Merged results\n",
        "    \"\"\"\n",
        "    if files is None:\n",
        "        files = sorted(output_dir.glob(\"full_results_*.json\"))\n",
        "    \n",
        "    if not files:\n",
        "        print(\"No files to merge.\")\n",
        "        return []\n",
        "    \n",
        "    print(f\"Merging {len(files)} files...\")\n",
        "    \n",
        "    all_results = []\n",
        "    for f in files:\n",
        "        try:\n",
        "            with open(f, 'r', encoding='utf-8') as fp:\n",
        "                data = json.load(fp)\n",
        "                if isinstance(data, list):\n",
        "                    all_results.extend(data)\n",
        "                    print(f\"  + {f.name}: {len(data)} results\")\n",
        "                else:\n",
        "                    print(f\"  ! {f.name}: unexpected format (not a list)\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ! {f.name}: error loading - {e}\")\n",
        "    \n",
        "    print(f\"\\nTotal: {len(all_results)} results\")\n",
        "    \n",
        "    # Deduplicate if requested\n",
        "    if deduplicate and all_results:\n",
        "        seen = set()\n",
        "        unique_results = []\n",
        "        for r in all_results:\n",
        "            # Create unique key from test_id, model, timestamp, and run_index\n",
        "            # run_index distinguishes multiple runs of the same test\n",
        "            key = (r.get(\"test_id\"), r.get(\"model\"), r.get(\"timestamp\"), r.get(\"run_index\", 0))\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                unique_results.append(r)\n",
        "        \n",
        "        removed = len(all_results) - len(unique_results)\n",
        "        if removed > 0:\n",
        "            print(f\"Deduplicated: removed {removed} duplicates, {len(unique_results)} unique results\")\n",
        "        all_results = unique_results\n",
        "    \n",
        "    # Save merged results\n",
        "    if output_name is None:\n",
        "        ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        output_name = f\"merged_results_{ts}.json\"\n",
        "    \n",
        "    output_path = output_dir / output_name\n",
        "    \n",
        "    def safe_serialize(obj):\n",
        "        if isinstance(obj, bytes):\n",
        "            return obj.decode('utf-8', errors='replace')\n",
        "        return str(obj)\n",
        "    \n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, indent=2, default=safe_serialize, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"\\nSaved to: {output_path}\")\n",
        "    \n",
        "    # Summary by model and service\n",
        "    if all_results:\n",
        "        df = pd.DataFrame(all_results)\n",
        "        print(\"\\n--- Summary by Model ---\")\n",
        "        model_summary = df.groupby(\"model\").agg(\n",
        "            tests=(\"passed\", \"count\"),\n",
        "            passed=(\"passed\", \"sum\"),\n",
        "            avg_score=(\"score\", \"mean\"),\n",
        "        ).reset_index()\n",
        "        model_summary[\"pass_rate\"] = (100 * model_summary[\"passed\"] / model_summary[\"tests\"]).round(1)\n",
        "        display(model_summary)\n",
        "        \n",
        "        if \"service\" in df.columns:\n",
        "            print(\"\\n--- Summary by Service ---\")\n",
        "            service_summary = df.groupby(\"service\").agg(\n",
        "                tests=(\"passed\", \"count\"),\n",
        "                passed=(\"passed\", \"sum\"),\n",
        "                avg_score=(\"score\", \"mean\"),\n",
        "            ).reset_index()\n",
        "            service_summary[\"pass_rate\"] = (100 * service_summary[\"passed\"] / service_summary[\"tests\"]).round(1)\n",
        "            display(service_summary)\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# files = list_result_files()\n",
        "# merged = merge_result_files()  # Merge all files\n",
        "# merged = merge_result_files(files=[files[0], files[2]])  # Merge specific files\n",
        "# merged = merge_result_files(deduplicate=True)  # Merge and remove duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List available result files\n",
        "files = list_result_files()\n",
        "\n",
        "#print(files)\n",
        "\n",
        "# Merge all files with deduplication\n",
        "merged = merge_result_files(deduplicate=False)\n",
        "\n",
        "# Alternative options:\n",
        "# merged = merge_result_files()  # Merge all without deduplication\n",
        "# merged = merge_result_files(files=[files[0], files[-1]])  # Merge specific files only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyze_results(merged)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
